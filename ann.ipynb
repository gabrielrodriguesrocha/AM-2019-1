{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2772, 3321)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np #importa a biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd #importa a biblioteca usada para trabalhar com dataframes\n",
    "import util\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "\n",
    "#importa o arquivo e extrai as features\n",
    "Xfeatures, Y = util.extract_features('datasets/everything.csv')\n",
    "print(Xfeatures.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Carregando parametros salvos da rede neural...\n",
      "\n",
      "Pesos carregados com sucesso!\n",
      "(25, 3322)\n",
      "(2, 26)\n",
      "(83102,)\n"
     ]
    }
   ],
   "source": [
    "# parametros a serem utilizados neste exercicio\n",
    "input_layer_size  = Xfeatures.shape[1]  # 20x20 dimensao das imagens de entrada\n",
    "hidden_layer_size = 25   # 25 neuronios na camada oculta\n",
    "num_labels = 2          # 10 rotulos, de 1 a 10  \n",
    "                         #  (observe que a classe \"0\" recebe o rotulo 10)\n",
    "    \n",
    "print('\\nCarregando parametros salvos da rede neural...\\n')\n",
    "\n",
    "# carregando os pesos da camada 1\n",
    "Theta1 = np.random.rand(hidden_layer_size, input_layer_size+1)*np.sqrt(1/(input_layer_size+hidden_layer_size))\n",
    "\n",
    "# carregando os pesos da camada 2\n",
    "Theta2 = np.random.rand(num_labels, hidden_layer_size+1)*np.sqrt(1/(hidden_layer_size+num_labels))\n",
    "\n",
    "# concatena os pesos em um único vetor\n",
    "nn_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "\n",
    "print('Pesos carregados com sucesso!')\n",
    "print(Theta1.shape)\n",
    "print(Theta2.shape)\n",
    "print(nn_params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcula a função sigmoidal  \n",
    "    \"\"\"\n",
    "\n",
    "    z = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Funcao de custo sem regularizacao ...\n",
      "\n",
      "Custo com os parametros (gerados aleatoriamente segundo inicialização de Xavier): 1.783894 \n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com duas camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # A variavel a seguir precisa ser retornada corretamente\n",
    "    J = 0;\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    \n",
    "    def l2a(i):\n",
    "        a = np.zeros(num_labels)\n",
    "        a[i] = 1\n",
    "        return a\n",
    "\n",
    "    Y = np.array([l2a(i) for i in y])\n",
    "    \n",
    "    a1 = np.insert(X.T, 0, 1,axis=0)\n",
    "    z2 = np.matmul(Theta1, a1)\n",
    "    a2 = np.insert(sigmoid(z2), 0, 1, axis=0)\n",
    "    z3 = np.matmul(Theta2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    J = 1/m * np.sum(np.sum(-Y * np.log(a3.T) - (1 - Y) * np.log(1 - a3.T)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "print('\\nFuncao de custo sem regularizacao ...\\n')\n",
    "\n",
    "J = funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, Xfeatures, Y)\n",
    "\n",
    "print('Custo com os parametros (gerados aleatoriamente segundo inicialização de Xavier): %1.6f ' %J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com duas camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # A variavel a seguir precisa ser retornada corretamente\n",
    "    J = 0;\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    #\n",
    "    # (3): Implemente a regularização na função de custo.\n",
    "    #\n",
    "\n",
    "    reg = vLambda / (2*m) * (np.sum(Theta1[:,1:] ** 2) + np.sum(Theta2[:,1:] ** 2))\n",
    "    J = funcaoCusto(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y) + reg\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checando a funcao de custo (c/ regularizacao) ... \n",
      "\n",
      "Custo com os parametros (inicializados aleatoriamente): 1.785482 \n"
     ]
    }
   ],
   "source": [
    "print('\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "# Parametro de regularizacao dos pesos (aqui sera igual a 1).\n",
    "vLambda = 1;\n",
    "\n",
    "J = funcaoCusto_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, Xfeatures, Y, vLambda)\n",
    "\n",
    "print('Custo com os parametros (inicializados aleatoriamente): %1.6f ' %J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inicializando parametros da rede neural...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inicializaPesosAleatorios(L_in, L_out, randomSeed = None):\n",
    "    '''\n",
    "    Inicializa aleatoriamente os pesos de uma camada usando \n",
    "    L_in (conexoes de entrada) e L_out (conexoes de saida).\n",
    "\n",
    "    W sera definido como uma matriz de dimensoes [L_out, 1 + L_in]\n",
    "    visto que devera armazenar os termos para \"bias\".\n",
    "    \n",
    "    randomSeed: indica a semente para o gerador aleatorio\n",
    "    '''\n",
    "\n",
    "    epsilon_init = 0.12\n",
    "    \n",
    "    # se for fornecida uma semente para o gerador aleatorio\n",
    "    if randomSeed is not None:\n",
    "        W = np.random.RandomState(randomSeed).rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    # se nao for fornecida uma semente para o gerador aleatorio\n",
    "    else:\n",
    "        W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    return W\n",
    "\n",
    "\n",
    "print('\\nInicializando parametros da rede neural...\\n')\n",
    "    \n",
    "#initial_Theta1 = inicializaPesosAleatorios(input_layer_size, hidden_layer_size, randomSeed = 10)\n",
    "#initial_Theta2 = inicializaPesosAleatorios(hidden_layer_size, num_labels, randomSeed = 20)\n",
    "\n",
    "# junta os pesos iniciais em um unico vetor\n",
    "#initial_rna_params = np.concatenate([np.ravel(initial_Theta1), np.ravel(initial_Theta2)])\n",
    "initial_rna_params = nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Avaliando o gradiente da sigmoide...\n",
      "\n",
      "Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\n",
      "\n",
      "[0.19661193 0.23500371 0.25       0.23500371 0.19661193]\n"
     ]
    }
   ],
   "source": [
    "def sigmoidGradient(z):\n",
    "    '''\n",
    "    Retorna o gradiente da funcao sigmoidal para z \n",
    "    \n",
    "    Calcula o gradiente da funcao sigmoidal\n",
    "    para z. A funcao deve funcionar independente se z for matriz ou vetor.\n",
    "    Nestes casos,  o gradiente deve ser calculado para cada elemento.\n",
    "    '''\n",
    "    \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Calcula o gradiente da funcao sigmoidal para \n",
    "    #           cada valor de z (seja z matriz, escalar ou vetor).\n",
    "    #\n",
    "\n",
    "    g = sigmoid(z) * (1 - sigmoid(z))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    ##########################################################################\n",
    "\n",
    "    return g\n",
    "\n",
    "print('\\nAvaliando o gradiente da sigmoide...\\n')\n",
    "\n",
    "g = sigmoidGradient(np.array([1,-0.5, 0, 0.5, 1]))\n",
    "print('Gradiente da sigmoide avaliado em [1 -0.5 0 0.5 1]:\\n')\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcaoCusto_backp(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para a tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    #\n",
    "\n",
    "    def l2a(i):\n",
    "        a = np.zeros(num_labels)\n",
    "        a[i] = 1\n",
    "        return a\n",
    "\n",
    "    Y = np.array([l2a(i) for i in y])\n",
    "    \n",
    "    a1 = np.insert(X.T, 0, 1,axis=0)\n",
    "    z2 = np.matmul(Theta1, a1)\n",
    "    a2 = np.insert(sigmoid(z2), 0, 1, axis=0)\n",
    "    z3 = np.matmul(Theta2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "\n",
    "    J = 1/m * np.sum(np.sum(-Y * np.log(a3.T) - (1 - Y) * np.log(1 - a3.T)))\n",
    "\n",
    "    d3 = a3 - Y.T\n",
    "    d2 = np.multiply(np.matmul(Theta2[:,1:].T, d3), sigmoidGradient(z2))\n",
    "\n",
    "    Theta1_grad = 1 / m * np.matmul(d2, a1.T)\n",
    "    Theta2_grad = 1 / m * np.matmul(d3, a2.T)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Checando a funcao de custo (c/ regularizacao) ... \n",
      "\n",
      "Custo com os parametros (carregados do arquivo): 1.788658\n"
     ]
    }
   ],
   "source": [
    "def funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    # (4): Implemente a regularização na função de custo e gradiente.\n",
    "    #\n",
    "\n",
    "    def l2a(i):\n",
    "        a = np.zeros(num_labels)\n",
    "        a[i] = 1\n",
    "        return a\n",
    "\n",
    "    Y = np.array([l2a(i) for i in y])\n",
    "    \n",
    "    a1 = np.insert(X.T, 0, 1,axis=0)\n",
    "    z2 = np.matmul(Theta1, a1)\n",
    "    a2 = np.insert(sigmoid(z2), 0, 1, axis=0)\n",
    "    z3 = np.matmul(Theta2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    reg = vLambda / (2*m) * (np.sum(Theta1[:,1:] ** 2) + np.sum(Theta2[:,1:] ** 2))\n",
    "    J = 1/m * np.sum(np.sum(-Y * np.log(a3.T) - (1 - Y) * np.log(1 - a3.T))) + reg\n",
    "\n",
    "    d3 = a3 - Y.T\n",
    "    d2 = np.multiply(np.matmul(Theta2[:,1:].T, d3), sigmoidGradient(z2))\n",
    "\n",
    "    Theta1_grad = 1 / m * np.matmul(d2, a1.T)\n",
    "    Theta2_grad = 1 / m * np.matmul(d3, a2.T)\n",
    "\n",
    "    reg1 = (vLambda / m) * Theta1\n",
    "    reg1[:,0] = 0\n",
    "    reg2 = (vLambda / m) * Theta2\n",
    "    reg2[:,0] = 0\n",
    "    \n",
    "    Theta1_grad = Theta1_grad + reg1\n",
    "    Theta2_grad = Theta2_grad + reg2\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parametro de regularizacao dos pesos.\n",
    "vLambda = 3;\n",
    "\n",
    "\n",
    "print('\\n\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "J, grad = funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, Xfeatures, Y, vLambda)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f' %J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando a rede neural.......\n",
      ".......(Aguarde, pois esse processo por ser um pouco demorado.)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.6565106395672943\n",
      "     jac: array([ 7.01918386e-03, -7.65869405e-05,  1.44721387e-05, ...,\n",
      "       -1.15399291e-02, -9.03281958e-03, -9.66976177e-03])\n",
      " message: 'Linear search failed'\n",
      "    nfev: 112\n",
      "     nit: 7\n",
      "  status: 4\n",
      " success: False\n",
      "       x: array([-0.04839451,  0.02651476,  0.00659818, ...,  0.15281782,\n",
      "       -0.42778196, -0.39935035])\n"
     ]
    }
   ],
   "source": [
    "print('\\nTreinando a rede neural.......')\n",
    "print('.......(Aguarde, pois esse processo por ser um pouco demorado.)\\n')\n",
    "\n",
    "# Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "# um valor maior e verifique como isso afeta o treinamento.\n",
    "MaxIter = 500\n",
    "\n",
    "# Voce tambem pode testar valores diferentes para lambda.\n",
    "vLambda = 1\n",
    "\n",
    "# Minimiza a funcao de custo\n",
    "result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, Xfeatures, Y, vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "# Coleta os pesos retornados pela função de minimização\n",
    "nn_params = result.x\n",
    "\n",
    "# Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0 1 0]\n",
      "[1 0 0 1 0]\n",
      "\n",
      "Acuracia no conjunto de treinamento: 88.023088\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predicao(Theta1, Theta2, X):\n",
    "    '''\n",
    "    Prediz o rotulo de uma amostra apresentada a rede neural\n",
    "    \n",
    "    Prediz o rotulo de X ao utilizar\n",
    "    os pesos treinados na rede neural (Theta1, Theta2)\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] # número de amostras\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    p = np.zeros(m)\n",
    "\n",
    "    a1 = np.hstack( [np.ones([m,1]),X] )\n",
    "    h1 = sigmoid( np.dot(a1,Theta1.T) )\n",
    "\n",
    "    a2 = np.hstack( [np.ones([m,1]),h1] ) \n",
    "    h2 = sigmoid( np.dot(a2,Theta2.T) )\n",
    "    \n",
    "    p = np.argmax(h2,axis=1)\n",
    "    #p = p+1\n",
    "    \n",
    "    return p\n",
    "    \n",
    "\n",
    "pred = predicao(Theta1, Theta2, Xfeatures)\n",
    "print(pred[:5])\n",
    "print(Y[:5])\n",
    "\n",
    "print('\\nAcuracia no conjunto de treinamento: %f\\n'%( np.mean( pred == Y ) * 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Theta1).to_csv('pesos_Theta1.csv', index=False)\n",
    "pd.DataFrame(Theta2).to_csv('pesos_Theta2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------\n",
      "1-fold: \n",
      "-----------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: divide by zero encountered in log\n",
      "D:\\Programs\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:61: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.902       0.888      0.895      0\n",
      "\t0.896       0.908      0.902      1\n",
      "\t------------------------------------------------\n",
      "\t0.899       0.898      0.898      Média macro\n",
      "\t0.899       0.899      0.899      Média micro\n",
      "\n",
      "\tAcuracia: 0.899\n",
      "\n",
      "-----------\n",
      "2-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.871       0.885      0.878      0\n",
      "\t0.896       0.884      0.890      1\n",
      "\t------------------------------------------------\n",
      "\t0.884       0.884      0.884      Média macro\n",
      "\t0.884       0.884      0.884      Média micro\n",
      "\n",
      "\tAcuracia: 0.884\n",
      "\n",
      "-----------\n",
      "3-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.871       0.891      0.881      0\n",
      "\t0.903       0.884      0.893      1\n",
      "\t------------------------------------------------\n",
      "\t0.887       0.888      0.887      Média macro\n",
      "\t0.888       0.888      0.888      Média micro\n",
      "\n",
      "\tAcuracia: 0.888\n",
      "\n",
      "-----------\n",
      "4-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.848       0.911      0.878      0\n",
      "\t0.924       0.869      0.896      1\n",
      "\t------------------------------------------------\n",
      "\t0.886       0.890      0.888      Média macro\n",
      "\t0.888       0.888      0.888      Média micro\n",
      "\n",
      "\tAcuracia: 0.888\n",
      "\n",
      "-----------\n",
      "5-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.909       0.876      0.892      0\n",
      "\t0.882       0.914      0.898      1\n",
      "\t------------------------------------------------\n",
      "\t0.896       0.895      0.895      Média macro\n",
      "\t0.895       0.895      0.895      Média micro\n",
      "\n",
      "\tAcuracia: 0.895\n",
      "\n",
      "-----------\n",
      "6-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.871       0.920      0.895      0\n",
      "\t0.931       0.887      0.908      1\n",
      "\t------------------------------------------------\n",
      "\t0.901       0.904      0.902      Média macro\n",
      "\t0.902       0.902      0.902      Média micro\n",
      "\n",
      "\tAcuracia: 0.902\n",
      "\n",
      "-----------\n",
      "7-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.879       0.835      0.856      0\n",
      "\t0.840       0.883      0.861      1\n",
      "\t------------------------------------------------\n",
      "\t0.860       0.859      0.859      Média macro\n",
      "\t0.859       0.859      0.859      Média micro\n",
      "\n",
      "\tAcuracia: 0.859\n",
      "\n",
      "-----------\n",
      "8-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.841       0.902      0.871      0\n",
      "\t0.917       0.863      0.889      1\n",
      "\t------------------------------------------------\n",
      "\t0.879       0.883      0.881      Média macro\n",
      "\t0.880       0.880      0.880      Média micro\n",
      "\n",
      "\tAcuracia: 0.880\n",
      "\n",
      "-----------\n",
      "9-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.818       0.857      0.837      0\n",
      "\t0.875       0.840      0.857      1\n",
      "\t------------------------------------------------\n",
      "\t0.847       0.849      0.848      Média macro\n",
      "\t0.848       0.848      0.848      Média micro\n",
      "\n",
      "\tAcuracia: 0.848\n",
      "\n",
      "-----------\n",
      "10-fold: \n",
      "-----------\n",
      "\n",
      "\n",
      "\tResultado no fold atual usando o melhor parametro encontrado:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.826       0.865      0.845      0\n",
      "\t0.882       0.847      0.864      1\n",
      "\t------------------------------------------------\n",
      "\t0.854       0.856      0.855      Média macro\n",
      "\t0.855       0.855      0.855      Média micro\n",
      "\n",
      "\tAcuracia: 0.855\n"
     ]
    }
   ],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "Xk, Yk = Xfeatures[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "nFolds = 10\n",
    "classes = [0,1]\n",
    "iteracoes=1000\n",
    "folds = util.stratified_kfolds(Yk, nFolds, classes) \n",
    "\n",
    "k=1\n",
    "resultados=[]\n",
    "for train_index, test_index in folds:\n",
    "    print('\\n-----------\\n%d-fold: \\n-----------\\n' % (k) )\n",
    "\n",
    "    # se train_index ou test_index forem vazios, interrompe o laco de repeticao\n",
    "    if len(train_index)==0 or len(test_index)==0: \n",
    "        print('\\tErro: o vetor com os indices de treinamento ou o vetor com os indices de teste esta vazio')      \n",
    "        break\n",
    "        \n",
    "    totalFold = len(train_index)+len(test_index)\n",
    "\n",
    "    X_train, X_test = Xk[train_index, :], Xk[test_index, :];\n",
    "    Y_train, Y_test = Yk[train_index], Yk[test_index];\n",
    "    \n",
    "    # separa os dados de treinamento em treinamento e validacao\n",
    "    pTrain = 0.8\n",
    "    train_index_v, val_index = util.stratified_holdOut(Y_train, pTrain)\n",
    "\n",
    "    # chama a função que faz a busca em grade\n",
    "    #bestRegularization = gridSearch(X_train_v, Y_train_v, X_val, Y_val)\n",
    "\n",
    "    # Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "    # um valor maior e verifique como isso afeta o treinamento.\n",
    "    MaxIter = 500\n",
    "\n",
    "    # Voce tambem pode testar valores diferentes para lambda.\n",
    "    vLambda = 1\n",
    "\n",
    "    # Minimiza a funcao de custo\n",
    "    result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, Xfeatures, Y, vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "    # Coleta os pesos retornados pela função de minimização\n",
    "    nn_params = result.x\n",
    "\n",
    "    # Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # classifica os dados de teste\n",
    "    Y_pred = predicao(Theta1, Theta2, X_test)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = util.get_confusionMatrix(Y_test, Y_pred, classes)\n",
    "\n",
    "    # Gera o relatório de desempenho\n",
    "    #print('\\n\\n\\n\\t'+\"=\"*50+'\\n\\tMelhor parametro de regularizacao: %1.6f' %bestRegularization)\n",
    "    print('\\n\\tResultado no fold atual usando o melhor parametro encontrado:')\n",
    "    auxResults = util.relatorioDesempenho(cm, classes, imprimeRelatorio=True)\n",
    "\n",
    "    # adiciona os resultados do fold atual na lista de resultados\n",
    "    resultados.append( auxResults ) \n",
    "        \n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultado final da classificação:\n",
      "\n",
      "\tRevocacao   Precisao   F-medida   Classe\n",
      "\t0.864       0.883      0.873      0\n",
      "\t0.894       0.878      0.886      1\n",
      "\t---------------------------------------------------------------------\n",
      "\t0.879       0.880      0.880      Média macro\n",
      "\t0.880       0.880      0.880      Média micro\n",
      "\n",
      "\tAcuracia: 0.880\n"
     ]
    }
   ],
   "source": [
    "print('\\nResultado final da classificação:')\n",
    "util.mediaFolds( resultados, classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de dados de validação: 444\n"
     ]
    }
   ],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "Xk, Yk = Xfeatures[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "# define a porcentagem de dados que irao compor o conjunto de treinamento\n",
    "pTrain = 0.8\n",
    "\n",
    "# obtem os indices dos dados da particao de treinamento e da particao de teste\n",
    "train_index, test_index = util.stratified_holdOut(Yk, pTrain)\n",
    "\n",
    "X_train, X_test = Xk[train_index, :], Xk[test_index, :];\n",
    "Y_train, Y_test = Yk[train_index], Yk[test_index];\n",
    "\n",
    "train_index, val_index = util.stratified_holdOut(Y_train, pTrain)\n",
    "\n",
    "X_train_v, X_val = X_train[train_index, :], X_train[val_index, :]\n",
    "Y_train_v, Y_val = Y_train[train_index], Y_train[val_index]\n",
    "\n",
    "print('Numero de dados de validação: %d' %(X_val.shape[0]))\n",
    "\n",
    "def curva_aprendizado(X, Y, Xval, Yval):\n",
    "    \"\"\"\n",
    "    Funcao usada gerar a curva de aprendizado.\n",
    "  \n",
    "    Parametros\n",
    "    ----------\n",
    "  \n",
    "    X : matriz com os dados de treinamento\n",
    "  \n",
    "    Y : vetor com as classes dos dados de treinamento\n",
    "  \n",
    "    Xval : matriz com os dados de validação\n",
    "  \n",
    "    Yval : vetor com as classes dos dados de validação\n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    # inicializa as listas que guardarao a performance no treinamento e na validacao\n",
    "    perf_train = []\n",
    "    perf_val = []\n",
    "\n",
    "    # inicializa o parametro de regularizacao da regressao logistica\n",
    "    lambda_reg = 1\n",
    "        \n",
    "    # Configura o numero de interacaoes da regressao logistica\n",
    "    iteracoes = 500\n",
    "        \n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ###############################\n",
    "    #  Instrucoes: Complete o codigo para gerar o gráfico da curva de aprendizado.\n",
    "    #           Comece o treinamento com as primeiras 10 amostras da base de dados de \n",
    "    #           treinamento e calcule a acuracia do classificador tanto nos dados de\n",
    "    #           treinamento já apresentados, quando na base de validacao. \n",
    "    #           Depois disso, adicione mais um dado para treinamento e calcule novamente \n",
    "    #           o desempenho. Continue adicionando um dado por vez ate todos os dados de \n",
    "    #           treinamento serem usados. Nas listas perf_train e perf_val, guarde a acuracia \n",
    "    #           obtida nos dados de treinamento e na base de validacao a cada nova adicao de \n",
    "    #           dados para treinamento.\n",
    "    \n",
    "    for i in np.arange(9, len(X) - 1):\n",
    "        # Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "        # um valor maior e verifique como isso afeta o treinamento.\n",
    "        MaxIter = 500\n",
    "\n",
    "        # Voce tambem pode testar valores diferentes para lambda.\n",
    "        vLambda = 1\n",
    "\n",
    "        # Minimiza a funcao de custo\n",
    "        result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, X[:i], Y[:i], vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "        # Coleta os pesos retornados pela função de minimização\n",
    "        nn_params = result.x\n",
    "\n",
    "        # Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "        Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "        Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "        \n",
    "        # Teste\n",
    "        p = predicao(Theta1, Theta2, X[:i])\n",
    "        cm = util.get_confusionMatrix(Y[:i], p, [0,1])\n",
    "        results = util.relatorioDesempenho(cm, [0,1])\n",
    "        perf_train.append(results['acuracia'])\n",
    "        # Validação\n",
    "        p = predicao(Theta1, Theta2, Xval[:i])\n",
    "        cm = util.get_confusionMatrix(Yval[:i], p, [0,1])\n",
    "        results = util.relatorioDesempenho(cm, [0,1])\n",
    "        perf_val.append(results['acuracia'])\n",
    "\n",
    "    ##################################################################################\n",
    "       \n",
    "    # Define o tamanho da figura \n",
    "    plt.figure(figsize=(20,12))\n",
    "\n",
    "    # Plota os dados\n",
    "    plt.plot(perf_train, color='blue', linestyle='-', linewidth=1.5, label='Treino') \n",
    "    plt.plot(perf_val, color='red', linestyle='-', linewidth=1.5, label='Validação')\n",
    "\n",
    "    # Define os nomes do eixo x e do eixo y\n",
    "    plt.xlabel(r'# Qtd. de dados de treinamento',fontsize='x-large') \n",
    "    plt.ylabel(r'Acuracia',fontsize='x-large') \n",
    "\n",
    "    # Define o título do gráfico\n",
    "    plt.title(r'Curva de aprendizado', fontsize='x-large')\n",
    "\n",
    "    # Acrescenta um grid no gráfico\n",
    "    plt.grid(axis='both')\n",
    "\n",
    "    # Plota a legenda\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "#curva_aprendizado(X_train_v, Y_train_v, X_val, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
