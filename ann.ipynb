{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np #importa a biblioteca usada para trabalhar com vetores e matrizes\n",
    "import pandas as pd #importa a biblioteca usada para trabalhar com dataframes\n",
    "import util\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "from numba import jit, njit\n",
    "import scipy.sparse as sparse\n",
    "from scipy.linalg.blas import sgemm\n",
    "import cProfile\n",
    "#import cupy as cp\n",
    "\n",
    "#importa o arquivo e extrai as features\n",
    "Xfeatures, Y = util.extract_features('datasets/clean_IPHONE6.csv', rep = 'ngrams', n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parametros a serem utilizados neste exercicio\n",
    "input_layer_size  = Xfeatures.shape[1]  # 20x20 dimensao das imagens de entrada\n",
    "hidden_layer_size = 500   # 25 neuronios na camada oculta\n",
    "num_labels = 2          # 10 rotulos, de 1 a 10  \n",
    "                         #  (observe que a classe \"0\" recebe o rotulo 10)\n",
    "    \n",
    "print('\\nCarregando parametros salvos da rede neural...\\n')\n",
    "\n",
    "# carregando os pesos da camada 1\n",
    "Theta1 = np.random.rand(hidden_layer_size, input_layer_size+1)*np.sqrt(1/(input_layer_size+hidden_layer_size))\n",
    "\n",
    "# carregando os pesos da camada 2\n",
    "Theta2 = np.random.rand(num_labels, hidden_layer_size+1)*np.sqrt(1/(hidden_layer_size+num_labels))\n",
    "\n",
    "# concatena os pesos em um único vetor\n",
    "nn_params = np.concatenate([np.ravel(Theta1), np.ravel(Theta2)])\n",
    "\n",
    "print('Pesos carregados com sucesso!')\n",
    "print(Theta1.shape)\n",
    "print(Theta2.shape)\n",
    "print(nn_params.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@njit\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Calcula a função sigmoidal  \n",
    "    \"\"\"\n",
    "\n",
    "    z = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inicializaPesosAleatorios(L_in, L_out, randomSeed = None):\n",
    "    '''\n",
    "    Inicializa aleatoriamente os pesos de uma camada usando \n",
    "    L_in (conexoes de entrada) e L_out (conexoes de saida).\n",
    "\n",
    "    W sera definido como uma matriz de dimensoes [L_out, 1 + L_in]\n",
    "    visto que devera armazenar os termos para \"bias\".\n",
    "    \n",
    "    randomSeed: indica a semente para o gerador aleatorio\n",
    "    '''\n",
    "\n",
    "    epsilon_init = 0.12\n",
    "    \n",
    "    # se for fornecida uma semente para o gerador aleatorio\n",
    "    if randomSeed is not None:\n",
    "        W = np.random.RandomState(randomSeed).rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    # se nao for fornecida uma semente para o gerador aleatorio\n",
    "    else:\n",
    "        W = np.random.rand(L_out, 1 + L_in) * 2 * epsilon_init - epsilon_init\n",
    "        \n",
    "    return W\n",
    "\n",
    "\n",
    "print('\\nInicializando parametros da rede neural...\\n')\n",
    "    \n",
    "#initial_Theta1 = inicializaPesosAleatorios(input_layer_size, hidden_layer_size, randomSeed = 10)\n",
    "#initial_Theta2 = inicializaPesosAleatorios(hidden_layer_size, num_labels, randomSeed = 20)\n",
    "\n",
    "# junta os pesos iniciais em um unico vetor\n",
    "#initial_rna_params = np.concatenate([np.ravel(initial_Theta1), np.ravel(initial_Theta2)])\n",
    "initial_rna_params = nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@njit\n",
    "def sigmoidGradient(z):\n",
    "    '''\n",
    "    Retorna o gradiente da funcao sigmoidal para z \n",
    "    \n",
    "    Calcula o gradiente da funcao sigmoidal\n",
    "    para z. A funcao deve funcionar independente se z for matriz ou vetor.\n",
    "    Nestes casos,  o gradiente deve ser calculado para cada elemento.\n",
    "    '''\n",
    "    \n",
    "    g = np.zeros(z.shape)\n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Calcula o gradiente da funcao sigmoidal para \n",
    "    #           cada valor de z (seja z matriz, escalar ou vetor).\n",
    "    #\n",
    "\n",
    "    g = sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2a(Y, num_labels):\n",
    "    n = len(Y)\n",
    "    y = np.zeros((n, num_labels))\n",
    "    y[np.arange(n), Y] = 1\n",
    "    return y\n",
    "\n",
    "def funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, X, Y, vLambda):\n",
    "    '''\n",
    "    Implementa a funcao de custo para a rede neural com tres camadas\n",
    "    voltada para tarefa de classificacao\n",
    "    \n",
    "    Calcula o custo e gradiente da rede neural. \n",
    "    Os parametros da rede neural sao colocados no vetor nn_params\n",
    "    e precisam ser transformados de volta nas matrizes de peso.\n",
    "    \n",
    "    input_layer_size - tamanho da camada de entrada\n",
    "    hidden_layer_size - tamanho da camada oculta\n",
    "    num_labels - numero de classes possiveis\n",
    "    lambda - parametro de regularizacao\n",
    "    \n",
    "    O vetor grad de retorno contem todas as derivadas parciais\n",
    "    da rede neural.\n",
    "    '''\n",
    "\n",
    "    # Extrai os parametros de nn_params e alimenta as variaveis Theta1 e Theta2.\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # Qtde de amostras\n",
    "    m = X.shape[0]\n",
    "         \n",
    "    # As variaveis a seguir precisam ser retornadas corretamente\n",
    "    J = 0;\n",
    "    Theta1_grad = np.zeros(Theta1.shape)\n",
    "    Theta2_grad = np.zeros(Theta2.shape)\n",
    "    \n",
    "\n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ########################\n",
    "    # Instrucoes: Voce deve completar o codigo a partir daqui \n",
    "    #               acompanhando os seguintes passos.\n",
    "    #\n",
    "    # (1): Lembre-se de transformar os rotulos Y em vetores com 10 posicoes,\n",
    "    #         onde tera zero em todas posicoes exceto na posicao do rotulo\n",
    "    #\n",
    "    # (2): Execute a etapa de feedforward e coloque o custo na variavel J.\n",
    "    #\n",
    "    # (3): Implemente o algoritmo de backpropagation para calcular \n",
    "    #      os gradientes e alimentar as variaveis Theta1_grad e Theta2_grad.\n",
    "    #\n",
    "    # (4): Implemente a regularização na função de custo e gradiente.\n",
    "    #\n",
    "    \n",
    "    a1 = np.vstack([np.ones(X.T.shape[1]), X.T])\n",
    "    z2 = np.matmul(Theta1, a1)\n",
    "    a2 = np.vstack([np.ones(z2.shape[1]), sigmoid(z2)])\n",
    "    z3 = np.matmul(Theta2, a2)\n",
    "    a3 = sigmoid(z3)\n",
    "    \n",
    "    reg = vLambda / (2*m) * (np.sum(Theta1[:,1:] ** 2) + np.sum(Theta2[:,1:] ** 2))\n",
    "    J = 1/m * np.sum(np.sum(-Y * np.log(a3.T) - (1 - Y) * np.log(1 - a3.T))) + reg\n",
    "\n",
    "    d3 = a3 - Y.T\n",
    "    d2 = np.multiply(np.matmul(Theta2[:,1:].T, d3), sigmoidGradient(z2))\n",
    "\n",
    "    Theta1_grad = 1 / m * np.matmul(d2, a1.T)\n",
    "    Theta2_grad = 1 / m * np.matmul(d3, a2.T)\n",
    "\n",
    "    reg1 = (vLambda / m) * Theta1\n",
    "    reg1[:,0] = 0\n",
    "    reg2 = (vLambda / m) * Theta2\n",
    "    reg2[:,0] = 0\n",
    "    \n",
    "    Theta1_grad = Theta1_grad + reg1\n",
    "    Theta2_grad = Theta2_grad + reg2\n",
    "\n",
    "    ##########################################################################\n",
    "\n",
    "    # Junta os gradientes\n",
    "    grad = np.concatenate([np.ravel(Theta1_grad), np.ravel(Theta2_grad)])\n",
    "\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parametro de regularizacao dos pesos.\n",
    "vLambda = 15;\n",
    "\n",
    "print('\\n\\nChecando a funcao de custo (c/ regularizacao) ... \\n')\n",
    "\n",
    "Yl = l2a(Y, num_labels)\n",
    "\n",
    "#cProfile.run('funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, Xfeatures, Yl, vLambda)')\n",
    "J, grad = funcaoCusto_backp_reg(nn_params, input_layer_size, hidden_layer_size, num_labels, Xfeatures, Yl, vLambda)\n",
    "\n",
    "print('Custo com os parametros (carregados do arquivo): %1.6f' %J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nTreinando a rede neural.......')\n",
    "print('.......(Aguarde, pois esse processo por ser um pouco demorado.)\\n')\n",
    "\n",
    "# Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "# um valor maior e verifique como isso afeta o treinamento.\n",
    "MaxIter = 500\n",
    "\n",
    "# Voce tambem pode testar valores diferentes para lambda.\n",
    "vLambda = 1\n",
    "\n",
    "# Minimiza a funcao de custo\n",
    "result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, Xfeatures, Yl, vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "# Coleta os pesos retornados pela função de minimização\n",
    "nn_params = result.x\n",
    "\n",
    "# Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicao(Theta1, Theta2, X):\n",
    "    '''\n",
    "    Prediz o rotulo de uma amostra apresentada a rede neural\n",
    "    \n",
    "    Prediz o rotulo de X ao utilizar\n",
    "    os pesos treinados na rede neural (Theta1, Theta2)\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] # número de amostras\n",
    "    num_labels = Theta2.shape[0]\n",
    "    \n",
    "    p = np.zeros(m)\n",
    "    \n",
    "    a1 = np.zeros((m,m+1))\n",
    "\n",
    "    a1 = np.hstack( [np.ones([m,1]),X] )\n",
    "    h1 = sigmoid( np.dot(a1,Theta1.T) )\n",
    "    \n",
    "    a2 = np.hstack( [np.ones([m,1]),h1] ) \n",
    "    h2 = sigmoid( np.dot(a2,Theta2.T) )\n",
    "    \n",
    "    p = np.argmax(h2,axis=1)\n",
    "    #p = p+1\n",
    "    \n",
    "    return p\n",
    "    \n",
    "\n",
    "pred = predicao(Theta1, Theta2, Xfeatures)\n",
    "print(pred[:5])\n",
    "print(Y[:5])\n",
    "\n",
    "print('\\nAcuracia no conjunto de treinamento: %f\\n'%( np.mean( pred == Y ) * 100) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame(Theta1).to_csv('ngrams_pesos_Theta1.csv', index=False)\n",
    "#pd.DataFrame(Theta2).to_csv('ngrams_pesos_Theta2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "Xk, Yk = Xfeatures[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "# define a porcentagem de dados que irao compor o conjunto de treinamento\n",
    "pTrain = 0.8\n",
    "\n",
    "# obtem os indices dos dados da particao de treinamento e da particao de teste\n",
    "train_index, test_index = util.stratified_holdOut(Yk, pTrain)\n",
    "\n",
    "X_train, X_test = Xk[train_index, :], Xk[test_index, :];\n",
    "Y_train, Y_test = Yk[train_index], Yk[test_index];\n",
    "\n",
    "train_index, val_index = util.stratified_holdOut(Y_train, pTrain)\n",
    "\n",
    "X_train_v, X_val = X_train[train_index, :], X_train[val_index, :]\n",
    "Y_train_v, Y_val = Y_train[train_index], Y_train[val_index]\n",
    "\n",
    "print('Numero de dados de validação: %d' %(X_val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [0,1]\n",
    "def gridSearch(X, Y, Xval, Yval):\n",
    "    \"\"\"\n",
    "    Retorna o melhor valor para os parametros lamba da regularizacao da Regressao Logistica.\n",
    "    \n",
    "    Parametros\n",
    "    ----------\n",
    "    X : matriz com os dados de treinamento\n",
    "    \n",
    "    Y : vetor com as classes dos dados de treinamento\n",
    "    \n",
    "    Xval : matriz com os dados de validacao\n",
    "    \n",
    "    Yval : vetor com as classes dos dados de validacao\n",
    "    \n",
    "    Retorno\n",
    "    -------\n",
    "    bestReg: o melhor valor para o parametro de regularizacao\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # inicializa a variável que deverá ser retornada pela função\n",
    "    bestReg = -100\n",
    "    \n",
    "    # Configura o numero de interacaoes da regressao logistica\n",
    "    iteracoes = 500\n",
    "    \n",
    "    # valores que deverao ser testados para o parametro de regularizacao \n",
    "    reg = [0,0.5,1,10,50,100];\n",
    "        \n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ###############################\n",
    "    # Instrucoes: Complete esta função para retornar os melhores valores do parametro\n",
    "    #             de regularizacao da regressao Logistica. \n",
    "    #\n",
    "    #             Você pode calcular o desempenho do classificador atraves da funcao\n",
    "    #             relatorioDesempenho() criada anteriormente. Use a acuracia para decidir\n",
    "    #             o melhor parametro.            \n",
    "    #\n",
    "    \n",
    "    perf = []\n",
    "\n",
    "    \n",
    "    def regmax(i):\n",
    "        result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, X, Y, i),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "        # Coleta os pesos retornados pela função de minimização\n",
    "        nn_params = result.x\n",
    "\n",
    "        # Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "        Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "        Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "        # classifica os dados de teste\n",
    "        Y_pred = predicao(Theta1, Theta2, X_val)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = util.get_confusionMatrix(Y_val, Y_pred, classes)\n",
    "        results = util.relatorioDesempenho(cm, [0,1])\n",
    "        return results['acuracia']\n",
    "    \n",
    "    vregmax = np.vectorize(regmax, otypes=['float'])\n",
    "    perf = vregmax(reg)\n",
    "    \n",
    "\n",
    "    '''\n",
    "    for i in reg:\n",
    "        result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, X, Y, i),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "        # Coleta os pesos retornados pela função de minimização\n",
    "        nn_params = result.x\n",
    "\n",
    "        # Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "        Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "        Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "        # classifica os dados de teste\n",
    "        Y_pred = predicao(Theta1, Theta2, X_val)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = util.get_confusionMatrix(Y_val, Y_pred, classes)\n",
    "        results = util.relatorioDesempenho(cm, [0,1])\n",
    "        return results['acuracia']\n",
    "        perf.append(results['acuracia'])\n",
    "    '''\n",
    "        \n",
    "    bestReg = reg[np.argmax(perf)]\n",
    "\n",
    "    ################################################################################## \n",
    "    \n",
    "    print(perf)\n",
    "    print(bestReg)\n",
    "\n",
    "    return bestReg\n",
    "\n",
    "\n",
    "gridSearch(X_train_v, l2a(Y_train_v, num_labels), X_val, Y_val)\n",
    "#print(bestLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curva_aprendizado(X, Y, Xval, Yval, Yl):\n",
    "    \"\"\"\n",
    "    Funcao usada gerar a curva de aprendizado.\n",
    "  \n",
    "    Parametros\n",
    "    ----------\n",
    "  \n",
    "    X : matriz com os dados de treinamento\n",
    "  \n",
    "    Y : vetor com as classes dos dados de treinamento\n",
    "  \n",
    "    Xval : matriz com os dados de validação\n",
    "  \n",
    "    Yval : vetor com as classes dos dados de validação\n",
    "  \n",
    "    \"\"\"\n",
    "\n",
    "    # inicializa as listas que guardarao a performance no treinamento e na validacao\n",
    "    perf_train = []\n",
    "    perf_val = []\n",
    "\n",
    "    # inicializa o parametro de regularizacao da regressao logistica\n",
    "    lambda_reg = 1\n",
    "        \n",
    "    # Configura o numero de interacaoes da regressao logistica\n",
    "    iteracoes = 500\n",
    "        \n",
    "    ########################## COMPLETE O CÓDIGO AQUI  ###############################\n",
    "    #  Instrucoes: Complete o codigo para gerar o gráfico da curva de aprendizado.\n",
    "    #           Comece o treinamento com as primeiras 10 amostras da base de dados de \n",
    "    #           treinamento e calcule a acuracia do classificador tanto nos dados de\n",
    "    #           treinamento já apresentados, quando na base de validacao. \n",
    "    #           Depois disso, adicione mais um dado para treinamento e calcule novamente \n",
    "    #           o desempenho. Continue adicionando um dado por vez ate todos os dados de \n",
    "    #           treinamento serem usados. Nas listas perf_train e perf_val, guarde a acuracia \n",
    "    #           obtida nos dados de treinamento e na base de validacao a cada nova adicao de \n",
    "    #           dados para treinamento.\n",
    "    \n",
    "    for i in np.arange(9, len(X) - 1):\n",
    "        print(i)\n",
    "        # Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "        # um valor maior e verifique como isso afeta o treinamento.\n",
    "        MaxIter = 500\n",
    "\n",
    "        # Voce tambem pode testar valores diferentes para lambda.\n",
    "        vLambda = bestLambda\n",
    "\n",
    "        # Minimiza a funcao de custo\n",
    "        result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, X[:i], Yl[:i], vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "        # Coleta os pesos retornados pela função de minimização\n",
    "        nn_params = result.x\n",
    "\n",
    "        # Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "        Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "        Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "        \n",
    "        # Teste\n",
    "        p = predicao(Theta1, Theta2, X[:i])\n",
    "        cm = util.get_confusionMatrix(Y[:i], p, [0,1])\n",
    "        results = util.relatorioDesempenho(cm, [0,1])\n",
    "        perf_train.append(results['acuracia'])\n",
    "        # Validação\n",
    "        p = predicao(Theta1, Theta2, Xval[:i])\n",
    "        cm = util.get_confusionMatrix(Yval[:i], p, [0,1])\n",
    "        results = util.relatorioDesempenho(cm, [0,1])\n",
    "        perf_val.append(results['acuracia'])\n",
    "\n",
    "    ##################################################################################\n",
    "       \n",
    "    # Define o tamanho da figura \n",
    "    plt.figure(figsize=(20,12))\n",
    "\n",
    "    # Plota os dados\n",
    "    plt.plot(perf_train, color='blue', linestyle='-', linewidth=1.5, label='Treino') \n",
    "    plt.plot(perf_val, color='red', linestyle='-', linewidth=1.5, label='Validação')\n",
    "\n",
    "    # Define os nomes do eixo x e do eixo y\n",
    "    plt.xlabel(r'# Qtd. de dados de treinamento',fontsize='x-large') \n",
    "    plt.ylabel(r'Acuracia',fontsize='x-large') \n",
    "\n",
    "    # Define o título do gráfico\n",
    "    plt.title(r'Curva de aprendizado', fontsize='x-large')\n",
    "\n",
    "    # Acrescenta um grid no gráfico\n",
    "    plt.grid(axis='both')\n",
    "\n",
    "    # Plota a legenda\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#curva_aprendizado(X_train_v, Y_train_v, X_val, Y_val, Yl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bestLambda = 0.5\n",
    "bestLambda = 15\n",
    "print(bestLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# semente usada na randomizacao dos dados.\n",
    "randomSeed = 10 \n",
    "\n",
    "# gera os indices aleatorios que irao definir a ordem dos dados\n",
    "idx_perm = np.random.RandomState(randomSeed).permutation(range(len(Y)))\n",
    "\n",
    "# ordena os dados de acordo com os indices gerados aleatoriamente\n",
    "Xk, Yk = Xfeatures[idx_perm, :], Y[idx_perm]\n",
    "\n",
    "nFolds = 10\n",
    "classes = [0,1]\n",
    "iteracoes=1000\n",
    "folds = util.stratified_kfolds(Yk, nFolds, classes) \n",
    "\n",
    "k=1\n",
    "resultados=[]\n",
    "for train_index, test_index in folds:\n",
    "    print('\\n-----------\\n%d-fold: \\n-----------\\n' % (k) )\n",
    "\n",
    "    # se train_index ou test_index forem vazios, interrompe o laco de repeticao\n",
    "    if len(train_index)==0 or len(test_index)==0: \n",
    "        print('\\tErro: o vetor com os indices de treinamento ou o vetor com os indices de teste esta vazio')      \n",
    "        break\n",
    "        \n",
    "    totalFold = len(train_index)+len(test_index)\n",
    "\n",
    "    X_train, X_test = Xk[train_index, :], Xk[test_index, :];\n",
    "    Y_train, Y_test = Yk[train_index], Yk[test_index];\n",
    "    \n",
    "    # separa os dados de treinamento em treinamento e validacao\n",
    "    pTrain = 0.8\n",
    "    train_index_v, val_index = util.stratified_holdOut(Y_train, pTrain)\n",
    "\n",
    "    # Apos ter completado toda a tarefa, mude o parametro MaxIter para\n",
    "    # um valor maior e verifique como isso afeta o treinamento.\n",
    "    MaxIter = 500\n",
    "\n",
    "    # Voce tambem pode testar valores diferentes para lambda.\n",
    "    #vLambda = 1\n",
    "    \n",
    "    # chama a função que faz a busca em grade\n",
    "    #vLambda = gridSearch(X_train_v, Y_train_v, X_val, Y_val)\n",
    "    vLambda = bestLambda\n",
    "\n",
    "    # Minimiza a funcao de custo\n",
    "    result = scipy.optimize.minimize(fun=funcaoCusto_backp_reg, x0=initial_rna_params, args=(input_layer_size, hidden_layer_size, num_labels, Xfeatures, Yl, vLambda),  \n",
    "                method='TNC', jac=True, options={'maxiter': MaxIter})\n",
    "\n",
    "    # Coleta os pesos retornados pela função de minimização\n",
    "    nn_params = result.x\n",
    "\n",
    "    # Obtem Theta1 e Theta2 back a partir de rna_params\n",
    "    Theta1 = np.reshape( nn_params[0:hidden_layer_size*(input_layer_size + 1)], (hidden_layer_size, input_layer_size+1) )\n",
    "    Theta2 = np.reshape( nn_params[ hidden_layer_size*(input_layer_size + 1):], (num_labels, hidden_layer_size+1) )\n",
    "\n",
    "    # classifica os dados de teste\n",
    "    Y_pred = predicao(Theta1, Theta2, X_test)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = util.get_confusionMatrix(Y_test, Y_pred, classes)\n",
    "\n",
    "    # Gera o relatório de desempenho\n",
    "    #print('\\n\\n\\n\\t'+\"=\"*50+'\\n\\tMelhor parametro de regularizacao: %1.6f' %bestRegularization)\n",
    "    print('\\n\\tResultado no fold atual usando o melhor parametro encontrado:')\n",
    "    auxResults = util.relatorioDesempenho(cm, classes, imprimeRelatorio=True)\n",
    "\n",
    "    # adiciona os resultados do fold atual na lista de resultados\n",
    "    resultados.append( auxResults ) \n",
    "        \n",
    "    k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('\\nResultado final da classificação:')\n",
    "util.mediaFolds( resultados, classes )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
