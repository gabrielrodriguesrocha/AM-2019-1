{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import util\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = nltk.word_tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the trianing parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=True, plot_cost=True):\n",
    "    \"\"\"\n",
    "    X: Input word indices. shape: (1, m)\n",
    "    Y: One-hot encoding of output word indices. shape: (vocab_size, m)\n",
    "    vocab_size: vocabulary size of your corpus or training data\n",
    "    emb_size: word embedding size. How many dimensions to represent each vocabulary\n",
    "    learning_rate: alaph in the weight update formula\n",
    "    epochs: how many epochs to train the model\n",
    "    batch_size: size of mini batch\n",
    "    parameters: pre-trained or pre-initialized parameters\n",
    "    print_cost: whether or not to print costs during the training process\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.756780275654817\n",
      "Cost after epoch 10: 2.7565498961762365\n",
      "Cost after epoch 20: 2.7563090534732457\n",
      "Cost after epoch 30: 2.756042579770476\n",
      "Cost after epoch 40: 2.7557351206886502\n",
      "Cost after epoch 50: 2.7553702395984505\n",
      "Cost after epoch 60: 2.754938642613247\n",
      "Cost after epoch 70: 2.7544166052928163\n",
      "Cost after epoch 80: 2.753782215006314\n",
      "Cost after epoch 90: 2.753011516270184\n",
      "Cost after epoch 100: 2.752077157331174\n",
      "Cost after epoch 110: 2.7509699636993\n",
      "Cost after epoch 120: 2.7496436067823486\n",
      "Cost after epoch 130: 2.7480573029661457\n",
      "Cost after epoch 140: 2.7461668051074164\n",
      "Cost after epoch 150: 2.7439217682316\n",
      "Cost after epoch 160: 2.741316883248447\n",
      "Cost after epoch 170: 2.738261396793704\n",
      "Cost after epoch 180: 2.734683784820115\n",
      "Cost after epoch 190: 2.7305110984664336\n",
      "Cost after epoch 200: 2.72566484121607\n",
      "Cost after epoch 210: 2.720170106454372\n",
      "Cost after epoch 220: 2.7138798468130556\n",
      "Cost after epoch 230: 2.7067057203788965\n",
      "Cost after epoch 240: 2.698575588832026\n",
      "Cost after epoch 250: 2.689428735989879\n",
      "Cost after epoch 260: 2.6794155396524104\n",
      "Cost after epoch 270: 2.6683886947751163\n",
      "Cost after epoch 280: 2.6563449926473437\n",
      "Cost after epoch 290: 2.6433392046103563\n",
      "Cost after epoch 300: 2.6294701828615517\n",
      "Cost after epoch 310: 2.6151509311750547\n",
      "Cost after epoch 320: 2.600349348379635\n",
      "Cost after epoch 330: 2.5852498067000185\n",
      "Cost after epoch 340: 2.57008808830329\n",
      "Cost after epoch 350: 2.5551071675835124\n",
      "Cost after epoch 360: 2.540796653050075\n",
      "Cost after epoch 370: 2.52710403258818\n",
      "Cost after epoch 380: 2.5141498760016536\n",
      "Cost after epoch 390: 2.502033786088738\n",
      "Cost after epoch 400: 2.4908006576605013\n",
      "Cost after epoch 410: 2.480621624376688\n",
      "Cost after epoch 420: 2.4712527627875187\n",
      "Cost after epoch 430: 2.4625865510185494\n",
      "Cost after epoch 440: 2.454512888455851\n",
      "Cost after epoch 450: 2.446907653028488\n",
      "Cost after epoch 460: 2.4397705913355523\n",
      "Cost after epoch 470: 2.4328550590337716\n",
      "Cost after epoch 480: 2.4260349229039475\n",
      "Cost after epoch 490: 2.419210213602512\n",
      "Cost after epoch 500: 2.4122947044733474\n",
      "Cost after epoch 510: 2.405346750113259\n",
      "Cost after epoch 520: 2.3982057949346665\n",
      "Cost after epoch 530: 2.3908221636919973\n",
      "Cost after epoch 540: 2.3831763441297853\n",
      "Cost after epoch 550: 2.3752650913629543\n",
      "Cost after epoch 560: 2.367250231540044\n",
      "Cost after epoch 570: 2.3590342020265584\n",
      "Cost after epoch 580: 2.3506381772579825\n",
      "Cost after epoch 590: 2.342108623727983\n",
      "Cost after epoch 600: 2.333498144699454\n",
      "Cost after epoch 610: 2.3250179071176325\n",
      "Cost after epoch 620: 2.3165825496561063\n",
      "Cost after epoch 630: 2.3082238908938986\n",
      "Cost after epoch 640: 2.299986374372402\n",
      "Cost after epoch 650: 2.2919085620825834\n",
      "Cost after epoch 660: 2.284163163343752\n",
      "Cost after epoch 670: 2.276642428341928\n",
      "Cost after epoch 680: 2.2693492026795283\n",
      "Cost after epoch 690: 2.2622975349498518\n",
      "Cost after epoch 700: 2.2554973497658546\n",
      "Cost after epoch 710: 2.2490710262430613\n",
      "Cost after epoch 720: 2.2429091917707837\n",
      "Cost after epoch 730: 2.2369995714175053\n",
      "Cost after epoch 740: 2.2313413263857322\n",
      "Cost after epoch 750: 2.2259318762209936\n",
      "Cost after epoch 760: 2.2208583049995294\n",
      "Cost after epoch 770: 2.216025001014033\n",
      "Cost after epoch 780: 2.211414889977688\n",
      "Cost after epoch 790: 2.207020450327361\n",
      "Cost after epoch 800: 2.20283324695146\n",
      "Cost after epoch 810: 2.1989145532813867\n",
      "Cost after epoch 820: 2.19518508033982\n",
      "Cost after epoch 830: 2.1916271296236935\n",
      "Cost after epoch 840: 2.188231060535445\n",
      "Cost after epoch 850: 2.184987250735357\n",
      "Cost after epoch 860: 2.181941144218865\n",
      "Cost after epoch 870: 2.1790299384623193\n",
      "Cost after epoch 880: 2.1762390839942145\n",
      "Cost after epoch 890: 2.1735609059260526\n",
      "Cost after epoch 900: 2.170988300642663\n",
      "Cost after epoch 910: 2.1685586980675606\n",
      "Cost after epoch 920: 2.166223805480723\n",
      "Cost after epoch 930: 2.163973668186442\n",
      "Cost after epoch 940: 2.1618040055743286\n",
      "Cost after epoch 950: 2.1597111219212572\n",
      "Cost after epoch 960: 2.1577277856197266\n",
      "Cost after epoch 970: 2.155816811656673\n",
      "Cost after epoch 980: 2.1539720233201503\n",
      "Cost after epoch 990: 2.1521917618339925\n",
      "Cost after epoch 1000: 2.1504747018721515\n",
      "Cost after epoch 1010: 2.1488492191222126\n",
      "Cost after epoch 1020: 2.147286044955799\n",
      "Cost after epoch 1030: 2.1457811803262596\n",
      "Cost after epoch 1040: 2.144334135893756\n",
      "Cost after epoch 1050: 2.1429444984841033\n",
      "Cost after epoch 1060: 2.141635538144978\n",
      "Cost after epoch 1070: 2.140383736171635\n",
      "Cost after epoch 1080: 2.139185975744778\n",
      "Cost after epoch 1090: 2.138041832188934\n",
      "Cost after epoch 1100: 2.1369508135608117\n",
      "Cost after epoch 1110: 2.1359307246774195\n",
      "Cost after epoch 1120: 2.1349626184477395\n",
      "Cost after epoch 1130: 2.134043622175714\n",
      "Cost after epoch 1140: 2.1331729081401853\n",
      "Cost after epoch 1150: 2.1323495444049265\n",
      "Cost after epoch 1160: 2.131586213552709\n",
      "Cost after epoch 1170: 2.130867896975436\n",
      "Cost after epoch 1180: 2.130191802542634\n",
      "Cost after epoch 1190: 2.129556656586694\n",
      "Cost after epoch 1200: 2.1289610900728473\n",
      "Cost after epoch 1210: 2.1284134726072765\n",
      "Cost after epoch 1220: 2.1279021636985496\n",
      "Cost after epoch 1230: 2.1274244498397823\n",
      "Cost after epoch 1240: 2.126978705011432\n",
      "Cost after epoch 1250: 2.12656322749099\n",
      "Cost after epoch 1260: 2.1261830853309003\n",
      "Cost after epoch 1270: 2.1258293885914967\n",
      "Cost after epoch 1280: 2.125499532209565\n",
      "Cost after epoch 1290: 2.1251916766330936\n",
      "Cost after epoch 1300: 2.1249039402108\n",
      "Cost after epoch 1310: 2.1246392180883644\n",
      "Cost after epoch 1320: 2.124390717014135\n",
      "Cost after epoch 1330: 2.124156025170794\n",
      "Cost after epoch 1340: 2.123933295123844\n",
      "Cost after epoch 1350: 2.1237206846935237\n",
      "Cost after epoch 1360: 2.123520083428111\n",
      "Cost after epoch 1370: 2.1233261730759065\n",
      "Cost after epoch 1380: 2.1231368585385937\n",
      "Cost after epoch 1390: 2.122950512947213\n",
      "Cost after epoch 1400: 2.1227655579577114\n",
      "Cost after epoch 1410: 2.122583908400814\n",
      "Cost after epoch 1420: 2.1224011345302505\n",
      "Cost after epoch 1430: 2.1222155435861243\n",
      "Cost after epoch 1440: 2.1220258925215343\n",
      "Cost after epoch 1450: 2.1218310122463078\n",
      "Cost after epoch 1460: 2.121633586079618\n",
      "Cost after epoch 1470: 2.121429541192821\n",
      "Cost after epoch 1480: 2.1212175961108573\n",
      "Cost after epoch 1490: 2.120996955445464\n",
      "Cost after epoch 1500: 2.120766905586456\n",
      "Cost after epoch 1510: 2.120531316687941\n",
      "Cost after epoch 1520: 2.1202860474308034\n",
      "Cost after epoch 1530: 2.120030179027593\n",
      "Cost after epoch 1540: 2.1197633508746385\n",
      "Cost after epoch 1550: 2.1194852806768316\n",
      "Cost after epoch 1560: 2.119201161143435\n",
      "Cost after epoch 1570: 2.1189065032640446\n",
      "Cost after epoch 1580: 2.1186006788647087\n",
      "Cost after epoch 1590: 2.1182837066102604\n",
      "Cost after epoch 1600: 2.1179556738071255\n",
      "Cost after epoch 1610: 2.1176230119246156\n",
      "Cost after epoch 1620: 2.117280757442241\n",
      "Cost after epoch 1630: 2.1169284984382877\n",
      "Cost after epoch 1640: 2.116566556514804\n",
      "Cost after epoch 1650: 2.116195308640087\n",
      "Cost after epoch 1660: 2.115822181063526\n",
      "Cost after epoch 1670: 2.115441741155682\n",
      "Cost after epoch 1680: 2.1150537243859406\n",
      "Cost after epoch 1690: 2.114658669080317\n",
      "Cost after epoch 1700: 2.1142571535398296\n",
      "Cost after epoch 1710: 2.1138572493283014\n",
      "Cost after epoch 1720: 2.113453160942224\n",
      "Cost after epoch 1730: 2.1130447105889174\n",
      "Cost after epoch 1740: 2.112632564861975\n",
      "Cost after epoch 1750: 2.112217414373654\n",
      "Cost after epoch 1760: 2.1118075739396596\n",
      "Cost after epoch 1770: 2.111397059548062\n",
      "Cost after epoch 1780: 2.1109857305347055\n",
      "Cost after epoch 1790: 2.110574299158831\n",
      "Cost after epoch 1800: 2.110163486924499\n",
      "Cost after epoch 1810: 2.109761446555069\n",
      "Cost after epoch 1820: 2.1093622178640103\n",
      "Cost after epoch 1830: 2.1089656620314674\n",
      "Cost after epoch 1840: 2.108572468010546\n",
      "Cost after epoch 1850: 2.108183321891833\n",
      "Cost after epoch 1860: 2.107805846271492\n",
      "Cost after epoch 1870: 2.107434328448275\n",
      "Cost after epoch 1880: 2.1070686123105924\n",
      "Cost after epoch 1890: 2.106709313244848\n",
      "Cost after epoch 1900: 2.106357035106551\n",
      "Cost after epoch 1910: 2.1060185670321068\n",
      "Cost after epoch 1920: 2.1056886562522363\n",
      "Cost after epoch 1930: 2.1053671253261865\n",
      "Cost after epoch 1940: 2.1050544855166993\n",
      "Cost after epoch 1950: 2.1047512312774295\n",
      "Cost after epoch 1960: 2.1044630927208843\n",
      "Cost after epoch 1970: 2.1041854606850245\n",
      "Cost after epoch 1980: 2.103918143687592\n",
      "Cost after epoch 1990: 2.1036615353205046\n",
      "Cost after epoch 2000: 2.1034160098264287\n",
      "Cost after epoch 2010: 2.1031860915741194\n",
      "Cost after epoch 2020: 2.102967958089503\n",
      "Cost after epoch 2030: 2.1027614172982396\n",
      "Cost after epoch 2040: 2.1025667438298097\n",
      "Cost after epoch 2050: 2.1023841922679143\n",
      "Cost after epoch 2060: 2.1022170066935773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2070: 2.102062247798388\n",
      "Cost after epoch 2080: 2.1019197375770577\n",
      "Cost after epoch 2090: 2.101789637688122\n",
      "Cost after epoch 2100: 2.1016720900780146\n",
      "Cost after epoch 2110: 2.1015690460743093\n",
      "Cost after epoch 2120: 2.1014784984860353\n",
      "Cost after epoch 2130: 2.101400295673946\n",
      "Cost after epoch 2140: 2.1013344957387243\n",
      "Cost after epoch 2150: 2.1012811378906755\n",
      "Cost after epoch 2160: 2.1012409199452806\n",
      "Cost after epoch 2170: 2.101212771158516\n",
      "Cost after epoch 2180: 2.1011965744479095\n",
      "Cost after epoch 2190: 2.1011922950020616\n",
      "Cost after epoch 2200: 2.1011998802030822\n",
      "Cost after epoch 2210: 2.10121885623408\n",
      "Cost after epoch 2220: 2.1012490538641795\n",
      "Cost after epoch 2230: 2.101290394582946\n",
      "Cost after epoch 2240: 2.1013427619126723\n",
      "Cost after epoch 2250: 2.101406022939216\n",
      "Cost after epoch 2260: 2.1014786491477153\n",
      "Cost after epoch 2270: 2.10156130527298\n",
      "Cost after epoch 2280: 2.1016539522676165\n",
      "Cost after epoch 2290: 2.1017564042074492\n",
      "Cost after epoch 2300: 2.1018684605352096\n",
      "Cost after epoch 2310: 2.1019876832286775\n",
      "Cost after epoch 2320: 2.1021154779488214\n",
      "Cost after epoch 2330: 2.102251844358055\n",
      "Cost after epoch 2340: 2.1023965409562075\n",
      "Cost after epoch 2350: 2.102549313982151\n",
      "Cost after epoch 2360: 2.102706982364131\n",
      "Cost after epoch 2370: 2.1028715812723995\n",
      "Cost after epoch 2380: 2.1030431479788843\n",
      "Cost after epoch 2390: 2.103221401296007\n",
      "Cost after epoch 2400: 2.103406050740566\n",
      "Cost after epoch 2410: 2.1035933520116634\n",
      "Cost after epoch 2420: 2.1037858468757\n",
      "Cost after epoch 2430: 2.1039836095005886\n",
      "Cost after epoch 2440: 2.1041863366267077\n",
      "Cost after epoch 2450: 2.1043937191307993\n",
      "Cost after epoch 2460: 2.1046016336879414\n",
      "Cost after epoch 2470: 2.104813000806959\n",
      "Cost after epoch 2480: 2.105027931302261\n",
      "Cost after epoch 2490: 2.1052461182032776\n",
      "Cost after epoch 2500: 2.105467252317807\n",
      "Cost after epoch 2510: 2.1056870102960272\n",
      "Cost after epoch 2520: 2.105908564757049\n",
      "Cost after epoch 2530: 2.1061320629482214\n",
      "Cost after epoch 2540: 2.1063572118558835\n",
      "Cost after epoch 2550: 2.1065837197723707\n",
      "Cost after epoch 2560: 2.106807227808489\n",
      "Cost after epoch 2570: 2.107031040930652\n",
      "Cost after epoch 2580: 2.107255341991978\n",
      "Cost after epoch 2590: 2.107479867377294\n",
      "Cost after epoch 2600: 2.107704357874057\n",
      "Cost after epoch 2610: 2.107924561027463\n",
      "Cost after epoch 2620: 2.108143806191952\n",
      "Cost after epoch 2630: 2.1083623098041357\n",
      "Cost after epoch 2640: 2.1085798495885144\n",
      "Cost after epoch 2650: 2.1087962100921174\n",
      "Cost after epoch 2660: 2.109007358925944\n",
      "Cost after epoch 2670: 2.1092165576517\n",
      "Cost after epoch 2680: 2.109424052759906\n",
      "Cost after epoch 2690: 2.1096296709796145\n",
      "Cost after epoch 2700: 2.109833247466878\n",
      "Cost after epoch 2710: 2.1100310520183574\n",
      "Cost after epoch 2720: 2.110226204783652\n",
      "Cost after epoch 2730: 2.110418977303065\n",
      "Cost after epoch 2740: 2.1106092485074024\n",
      "Cost after epoch 2750: 2.110796906518633\n",
      "Cost after epoch 2760: 2.1109785734881568\n",
      "Cost after epoch 2770: 2.1111571745363125\n",
      "Cost after epoch 2780: 2.1113330001158657\n",
      "Cost after epoch 2790: 2.1115059804319016\n",
      "Cost after epoch 2800: 2.111676054877155\n",
      "Cost after epoch 2810: 2.111840218163893\n",
      "Cost after epoch 2820: 2.1120011668443066\n",
      "Cost after epoch 2830: 2.1121592035568706\n",
      "Cost after epoch 2840: 2.11231430546708\n",
      "Cost after epoch 2850: 2.112466458297809\n",
      "Cost after epoch 2860: 2.112613023425126\n",
      "Cost after epoch 2870: 2.112756454199168\n",
      "Cost after epoch 2880: 2.1128970587449687\n",
      "Cost after epoch 2890: 2.11303485444749\n",
      "Cost after epoch 2900: 2.1131698661684313\n",
      "Cost after epoch 2910: 2.1132997950081887\n",
      "Cost after epoch 2920: 2.1134268531948663\n",
      "Cost after epoch 2930: 2.1135513482430786\n",
      "Cost after epoch 2940: 2.113673329603099\n",
      "Cost after epoch 2950: 2.113792852853253\n",
      "Cost after epoch 2960: 2.113907916430514\n",
      "Cost after epoch 2970: 2.1140205073310128\n",
      "Cost after epoch 2980: 2.11413092741403\n",
      "Cost after epoch 2990: 2.114239249563778\n",
      "Cost after epoch 3000: 2.114345551341628\n",
      "Cost after epoch 3010: 2.114448076233062\n",
      "Cost after epoch 3020: 2.1145486124149473\n",
      "Cost after epoch 3030: 2.1146474523806305\n",
      "Cost after epoch 3040: 2.1147446841103124\n",
      "Cost after epoch 3050: 2.1148403988434055\n",
      "Cost after epoch 3060: 2.114933027984138\n",
      "Cost after epoch 3070: 2.1150241944645845\n",
      "Cost after epoch 3080: 2.1151141791429895\n",
      "Cost after epoch 3090: 2.1152030776131583\n",
      "Cost after epoch 3100: 2.1152909874411017\n",
      "Cost after epoch 3110: 2.1153764702569844\n",
      "Cost after epoch 3120: 2.1154610213091107\n",
      "Cost after epoch 3130: 2.1155449089439315\n",
      "Cost after epoch 3140: 2.1156282300897598\n",
      "Cost after epoch 3150: 2.1157110825516745\n",
      "Cost after epoch 3160: 2.1157921032826\n",
      "Cost after epoch 3170: 2.115872698595899\n",
      "Cost after epoch 3180: 2.115953124672627\n",
      "Cost after epoch 3190: 2.1160334748540732\n",
      "Cost after epoch 3200: 2.1161138424819286\n",
      "Cost after epoch 3210: 2.116192890120612\n",
      "Cost after epoch 3220: 2.116271972780851\n",
      "Cost after epoch 3230: 2.116351335800402\n",
      "Cost after epoch 3240: 2.116431065367807\n",
      "Cost after epoch 3250: 2.1165112470158793\n",
      "Cost after epoch 3260: 2.116590525919224\n",
      "Cost after epoch 3270: 2.1166702405420406\n",
      "Cost after epoch 3280: 2.1167506273930177\n",
      "Cost after epoch 3290: 2.1168317631844156\n",
      "Cost after epoch 3300: 2.116913723519359\n",
      "Cost after epoch 3310: 2.1169951006494783\n",
      "Cost after epoch 3320: 2.117077247151547\n",
      "Cost after epoch 3330: 2.117160393127715\n",
      "Cost after epoch 3340: 2.11724460455055\n",
      "Cost after epoch 3350: 2.117329946004036\n",
      "Cost after epoch 3360: 2.1174149289033695\n",
      "Cost after epoch 3370: 2.1175009457709955\n",
      "Cost after epoch 3380: 2.1175882228761287\n",
      "Cost after epoch 3390: 2.117676815035555\n",
      "Cost after epoch 3400: 2.1177667755382448\n",
      "Cost after epoch 3410: 2.117856514372382\n",
      "Cost after epoch 3420: 2.1179474847028175\n",
      "Cost after epoch 3430: 2.1180399114770383\n",
      "Cost after epoch 3440: 2.118133838572908\n",
      "Cost after epoch 3450: 2.118229308307484\n",
      "Cost after epoch 3460: 2.1183246153756596\n",
      "Cost after epoch 3470: 2.118421289167764\n",
      "Cost after epoch 3480: 2.118519555596655\n",
      "Cost after epoch 3490: 2.118619448255051\n",
      "Cost after epoch 3500: 2.118720999217129\n",
      "Cost after epoch 3510: 2.118822379966984\n",
      "Cost after epoch 3520: 2.1189252068586786\n",
      "Cost after epoch 3530: 2.1190297087240313\n",
      "Cost after epoch 3540: 2.1191359097950992\n",
      "Cost after epoch 3550: 2.1192438328766565\n",
      "Cost after epoch 3560: 2.119351523419484\n",
      "Cost after epoch 3570: 2.119460691069468\n",
      "Cost after epoch 3580: 2.1195715691357027\n",
      "Cost after epoch 3590: 2.1196841735528498\n",
      "Cost after epoch 3600: 2.1197985189477206\n",
      "Cost after epoch 3610: 2.1199125261506957\n",
      "Cost after epoch 3620: 2.120028000549001\n",
      "Cost after epoch 3630: 2.1201451810700345\n",
      "Cost after epoch 3640: 2.1202640764550584\n",
      "Cost after epoch 3650: 2.120384694269529\n",
      "Cost after epoch 3660: 2.120504835555344\n",
      "Cost after epoch 3670: 2.1206264005853703\n",
      "Cost after epoch 3680: 2.120749634637679\n",
      "Cost after epoch 3690: 2.1208745403369758\n",
      "Cost after epoch 3700: 2.121001119265982\n",
      "Cost after epoch 3710: 2.121127060185209\n",
      "Cost after epoch 3720: 2.121254354666053\n",
      "Cost after epoch 3730: 2.121383254691152\n",
      "Cost after epoch 3740: 2.1215137577754892\n",
      "Cost after epoch 3750: 2.1216458605206445\n",
      "Cost after epoch 3760: 2.1217771489313724\n",
      "Cost after epoch 3770: 2.121909700104689\n",
      "Cost after epoch 3780: 2.1220437727468973\n",
      "Cost after epoch 3790: 2.1221793601765357\n",
      "Cost after epoch 3800: 2.122316454918049\n",
      "Cost after epoch 3810: 2.122452551199009\n",
      "Cost after epoch 3820: 2.1225898041828763\n",
      "Cost after epoch 3830: 2.122728479034439\n",
      "Cost after epoch 3840: 2.122868565685999\n",
      "Cost after epoch 3850: 2.1230100533838976\n",
      "Cost after epoch 3860: 2.12315035653247\n",
      "Cost after epoch 3870: 2.123291699660167\n",
      "Cost after epoch 3880: 2.123434353893118\n",
      "Cost after epoch 3890: 2.123578306484451\n",
      "Cost after epoch 3900: 2.1237235440978495\n",
      "Cost after epoch 3910: 2.123867413846974\n",
      "Cost after epoch 3920: 2.12401220006679\n",
      "Cost after epoch 3930: 2.124158179166195\n",
      "Cost after epoch 3940: 2.1243053363288307\n",
      "Cost after epoch 3950: 2.1244536562341914\n",
      "Cost after epoch 3960: 2.124600431491421\n",
      "Cost after epoch 3970: 2.124747996154074\n",
      "Cost after epoch 3980: 2.12489663110036\n",
      "Cost after epoch 3990: 2.125046319965556\n",
      "Cost after epoch 4000: 2.1251970459558165\n",
      "Cost after epoch 4010: 2.1253460599525194\n",
      "Cost after epoch 4020: 2.1254957353841393\n",
      "Cost after epoch 4030: 2.1256463566910844\n",
      "Cost after epoch 4040: 2.125797906402915\n",
      "Cost after epoch 4050: 2.125950366685899\n",
      "Cost after epoch 4060: 2.1261009592508064\n",
      "Cost after epoch 4070: 2.1262520865266294\n",
      "Cost after epoch 4080: 2.1264040355507112\n",
      "Cost after epoch 4090: 2.126556788121101\n",
      "Cost after epoch 4100: 2.1267103257303335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 4110: 2.126861853090333\n",
      "Cost after epoch 4120: 2.1270137914075864\n",
      "Cost after epoch 4130: 2.127166429322768\n",
      "Cost after epoch 4140: 2.1273197482175146\n",
      "Cost after epoch 4150: 2.1274737292187367\n",
      "Cost after epoch 4160: 2.127625571697219\n",
      "Cost after epoch 4170: 2.1277777057103044\n",
      "Cost after epoch 4180: 2.127930420504497\n",
      "Cost after epoch 4190: 2.128083697309337\n",
      "Cost after epoch 4200: 2.128237517144347\n",
      "Cost after epoch 4210: 2.128389085085097\n",
      "Cost after epoch 4220: 2.1285408305228097\n",
      "Cost after epoch 4230: 2.128693042328113\n",
      "Cost after epoch 4240: 2.1288457017991447\n",
      "Cost after epoch 4250: 2.128998790063454\n",
      "Cost after epoch 4260: 2.1291495282635906\n",
      "Cost after epoch 4270: 2.1293003361037988\n",
      "Cost after epoch 4280: 2.1294515011284227\n",
      "Cost after epoch 4290: 2.1296030048873806\n",
      "Cost after epoch 4300: 2.1297548287948023\n",
      "Cost after epoch 4310: 2.129904219688265\n",
      "Cost after epoch 4320: 2.130053579124148\n",
      "Cost after epoch 4330: 2.1302031924133695\n",
      "Cost after epoch 4340: 2.1303530415084593\n",
      "Cost after epoch 4350: 2.1305031082568733\n",
      "Cost after epoch 4360: 2.130650674052635\n",
      "Cost after epoch 4370: 2.1307981144483024\n",
      "Cost after epoch 4380: 2.1309457116664756\n",
      "Cost after epoch 4390: 2.1310934481851325\n",
      "Cost after epoch 4400: 2.1312413064042683\n",
      "Cost after epoch 4410: 2.13138661035168\n",
      "Cost after epoch 4420: 2.13153170235423\n",
      "Cost after epoch 4430: 2.1316768607500642\n",
      "Cost after epoch 4440: 2.1318220686411733\n",
      "Cost after epoch 4450: 2.131967309075417\n",
      "Cost after epoch 4460: 2.132109956002623\n",
      "Cost after epoch 4470: 2.1322523119521897\n",
      "Cost after epoch 4480: 2.132394650644886\n",
      "Cost after epoch 4490: 2.1325369558842056\n",
      "Cost after epoch 4500: 2.1326792114404456\n",
      "Cost after epoch 4510: 2.132818847690063\n",
      "Cost after epoch 4520: 2.132958121448417\n",
      "Cost after epoch 4530: 2.1330973011519903\n",
      "Cost after epoch 4540: 2.1332363713649483\n",
      "Cost after epoch 4550: 2.13337531663659\n",
      "Cost after epoch 4560: 2.1335116295048717\n",
      "Cost after epoch 4570: 2.133647515805845\n",
      "Cost after epoch 4580: 2.1337832380921866\n",
      "Cost after epoch 4590: 2.1339187817320275\n",
      "Cost after epoch 4600: 2.134054132094598\n",
      "Cost after epoch 4610: 2.1341868488653915\n",
      "Cost after epoch 4620: 2.1343190822759155\n",
      "Cost after epoch 4630: 2.134451088463167\n",
      "Cost after epoch 4640: 2.1345828536287854\n",
      "Cost after epoch 4650: 2.134714363989335\n",
      "Cost after epoch 4660: 2.134843250641514\n",
      "Cost after epoch 4670: 2.1349716042098077\n",
      "Cost after epoch 4680: 2.1350996739506427\n",
      "Cost after epoch 4690: 2.1352274469168244\n",
      "Cost after epoch 4700: 2.135354910187978\n",
      "Cost after epoch 4710: 2.1354797698441503\n",
      "Cost after epoch 4720: 2.13560405350104\n",
      "Cost after epoch 4730: 2.1357280031351307\n",
      "Cost after epoch 4740: 2.1358516066578215\n",
      "Cost after epoch 4750: 2.1359748520174864\n",
      "Cost after epoch 4760: 2.136095523192339\n",
      "Cost after epoch 4770: 2.136215581962402\n",
      "Cost after epoch 4780: 2.136335262688303\n",
      "Cost after epoch 4790: 2.136454554138694\n",
      "Cost after epoch 4800: 2.136573445127762\n",
      "Cost after epoch 4810: 2.136689799826064\n",
      "Cost after epoch 4820: 2.1368055118972147\n",
      "Cost after epoch 4830: 2.1369208078109736\n",
      "Cost after epoch 4840: 2.1370356771843975\n",
      "Cost after epoch 4850: 2.1371501096872003\n",
      "Cost after epoch 4860: 2.1372620513942073\n",
      "Cost after epoch 4870: 2.1373733260871886\n",
      "Cost after epoch 4880: 2.137484152127984\n",
      "Cost after epoch 4890: 2.1375945199668305\n",
      "Cost after epoch 4900: 2.1377044201124242\n",
      "Cost after epoch 4910: 2.1378118817130214\n",
      "Cost after epoch 4920: 2.1379186573860594\n",
      "Cost after epoch 4930: 2.138024957223141\n",
      "Cost after epoch 4940: 2.1381307724870524\n",
      "Cost after epoch 4950: 2.1382360945036467\n",
      "Cost after epoch 4960: 2.1383390361610526\n",
      "Cost after epoch 4970: 2.1384412780795463\n",
      "Cost after epoch 4980: 2.1385430219694643\n",
      "Cost after epoch 4990: 2.138644259880986\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x282a04d4048>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loser's neighbor words: ['.', 'a', 'game', \"'s\"]\n",
      "beating's neighbor words: ['market', 'stock', 'the', 'investing']\n",
      "investing's neighbor words: ['the', 'of', ',', 'beating']\n",
      ",'s neighbor words: ['stock', 'the', 'beating', 'costs']\n",
      "stock's neighbor words: ['a', 'is', ',', 'market']\n",
      "of's neighbor words: ['the', 'costs', 'beating', 'of']\n",
      ".'s neighbor words: ['loser', \"'s\", 'game', 'stock']\n",
      "market's neighbor words: ['loser', 'stock', 'is', 'beating']\n",
      "game's neighbor words: ['.', 'a', 'game', \"'s\"]\n",
      "After's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "deduction's neighbor words: ['costs', 'After', 'the', 'of']\n",
      "the's neighbor words: ['is', 'market', 'the', 'stock']\n",
      "is's neighbor words: ['loser', \"'s\", 'market', 'a']\n",
      "'s's neighbor words: ['.', 'a', 'game', \"'s\"]\n",
      "a's neighbor words: ['loser', \"'s\", 'game', 'stock']\n",
      "costs's neighbor words: [',', 'of', 'the', 'deduction']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
