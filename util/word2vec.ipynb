{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def mapping(tokens):\n",
    "    word_to_id = dict()\n",
    "    id_to_word = dict()\n",
    "\n",
    "    for i, token in enumerate(set(tokens)):\n",
    "        word_to_id[token] = i\n",
    "        id_to_word[i] = token\n",
    "\n",
    "    return word_to_id, id_to_word\n",
    "\n",
    "def generate_training_data(tokens, word_to_id, window_size):\n",
    "    N = len(tokens)\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(N):\n",
    "        nbr_inds = list(range(max(0, i - window_size), i)) + \\\n",
    "                   list(range(i + 1, min(N, i + window_size + 1)))\n",
    "        for j in nbr_inds:\n",
    "            X.append(word_to_id[tokens[i]])\n",
    "            Y.append(word_to_id[tokens[j]])\n",
    "            \n",
    "    X = np.array(X)\n",
    "    X = np.expand_dims(X, axis=0)\n",
    "    Y = np.array(Y)\n",
    "    Y = np.expand_dims(Y, axis=0)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_wrd_emb(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    vocab_size: int. vocabulary size of your corpus or training data\n",
    "    emb_size: int. word embedding size. How many dimensions to represent each vocabulary\n",
    "    \"\"\"\n",
    "    WRD_EMB = np.random.randn(vocab_size, emb_size) * 0.01\n",
    "    return WRD_EMB\n",
    "\n",
    "def initialize_dense(input_size, output_size):\n",
    "    \"\"\"\n",
    "    input_size: int. size of the input to the dense layer\n",
    "    output_szie: int. size of the output out of the dense layer\n",
    "    \"\"\"\n",
    "    W = np.random.randn(output_size, input_size) * 0.01\n",
    "    return W\n",
    "\n",
    "def initialize_parameters(vocab_size, emb_size):\n",
    "    \"\"\"\n",
    "    initialize all the trianing parameters\n",
    "    \"\"\"\n",
    "    WRD_EMB = initialize_wrd_emb(vocab_size, emb_size)\n",
    "    W = initialize_dense(emb_size, vocab_size)\n",
    "    \n",
    "    parameters = {}\n",
    "    parameters['WRD_EMB'] = WRD_EMB\n",
    "    parameters['W'] = W\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_to_word_vecs(inds, parameters):\n",
    "    \"\"\"\n",
    "    inds: numpy array. shape: (1, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = inds.shape[1]\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    word_vec = WRD_EMB[inds.flatten(), :].T\n",
    "    \n",
    "    assert(word_vec.shape == (WRD_EMB.shape[1], m))\n",
    "    \n",
    "    return word_vec\n",
    "\n",
    "def linear_dense(word_vec, parameters):\n",
    "    \"\"\"\n",
    "    word_vec: numpy array. shape: (emb_size, m)\n",
    "    parameters: dict. weights to be trained\n",
    "    \"\"\"\n",
    "    m = word_vec.shape[1]\n",
    "    W = parameters['W']\n",
    "    Z = np.dot(W, word_vec)\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], m))\n",
    "    \n",
    "    return W, Z\n",
    "\n",
    "def softmax(Z):\n",
    "    \"\"\"\n",
    "    Z: output out of the dense layer. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    softmax_out = np.divide(np.exp(Z), np.sum(np.exp(Z), axis=0, keepdims=True) + 0.001)\n",
    "    \n",
    "    assert(softmax_out.shape == Z.shape)\n",
    "\n",
    "    return softmax_out\n",
    "\n",
    "def forward_propagation(inds, parameters):\n",
    "    word_vec = ind_to_word_vecs(inds, parameters)\n",
    "    W, Z = linear_dense(word_vec, parameters)\n",
    "    softmax_out = softmax(Z)\n",
    "    \n",
    "    caches = {}\n",
    "    caches['inds'] = inds\n",
    "    caches['word_vec'] = word_vec\n",
    "    caches['W'] = W\n",
    "    caches['Z'] = Z\n",
    "    \n",
    "    return softmax_out, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(softmax_out, Y):\n",
    "    \"\"\"\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    m = softmax_out.shape[1]\n",
    "    cost = -(1 / m) * np.sum(np.sum(Y * np.log(softmax_out + 0.001), axis=0, keepdims=True), axis=1)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backward(Y, softmax_out):\n",
    "    \"\"\"\n",
    "    Y: labels of training data. shape: (vocab_size, m)\n",
    "    softmax_out: output out of softmax. shape: (vocab_size, m)\n",
    "    \"\"\"\n",
    "    dL_dZ = softmax_out - Y\n",
    "    \n",
    "    assert(dL_dZ.shape == softmax_out.shape)\n",
    "    return dL_dZ\n",
    "\n",
    "def dense_backward(dL_dZ, caches):\n",
    "    \"\"\"\n",
    "    dL_dZ: shape: (vocab_size, m)\n",
    "    caches: dict. results from each steps of forward propagation\n",
    "    \"\"\"\n",
    "    W = caches['W']\n",
    "    word_vec = caches['word_vec']\n",
    "    m = word_vec.shape[1]\n",
    "    \n",
    "    dL_dW = (1 / m) * np.dot(dL_dZ, word_vec.T)\n",
    "    dL_dword_vec = np.dot(W.T, dL_dZ)\n",
    "\n",
    "    assert(W.shape == dL_dW.shape)\n",
    "    assert(word_vec.shape == dL_dword_vec.shape)\n",
    "    \n",
    "    return dL_dW, dL_dword_vec\n",
    "\n",
    "def backward_propagation(Y, softmax_out, caches):\n",
    "    dL_dZ = softmax_backward(Y, softmax_out)\n",
    "    dL_dW, dL_dword_vec = dense_backward(dL_dZ, caches)\n",
    "    \n",
    "    gradients = dict()\n",
    "    gradients['dL_dZ'] = dL_dZ\n",
    "    gradients['dL_dW'] = dL_dW\n",
    "    gradients['dL_dword_vec'] = dL_dword_vec\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def update_parameters(parameters, caches, gradients, learning_rate):\n",
    "    vocab_size, emb_size = parameters['WRD_EMB'].shape\n",
    "    inds = caches['inds']\n",
    "    WRD_EMB = parameters['WRD_EMB']\n",
    "    dL_dword_vec = gradients['dL_dword_vec']\n",
    "    m = inds.shape[-1]\n",
    "    \n",
    "    WRD_EMB[inds.flatten(), :] -= dL_dword_vec.T * learning_rate\n",
    "\n",
    "    parameters['W'] -= learning_rate * gradients['dL_dW']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_model_training(X, Y, vocab_size, emb_size, learning_rate, epochs, batch_size=256, parameters=None, print_cost=True, plot_cost=True):\n",
    "    \"\"\"\n",
    "    X: Input word indices. shape: (1, m)\n",
    "    Y: One-hot encoding of output word indices. shape: (vocab_size, m)\n",
    "    vocab_size: vocabulary size of your corpus or training data\n",
    "    emb_size: word embedding size. How many dimensions to represent each vocabulary\n",
    "    learning_rate: alaph in the weight update formula\n",
    "    epochs: how many epochs to train the model\n",
    "    batch_size: size of mini batch\n",
    "    parameters: pre-trained or pre-initialized parameters\n",
    "    print_cost: whether or not to print costs during the training process\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    if parameters is None:\n",
    "        parameters = initialize_parameters(vocab_size, emb_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_cost = 0\n",
    "        batch_inds = list(range(0, m, batch_size))\n",
    "        np.random.shuffle(batch_inds)\n",
    "        for i in batch_inds:\n",
    "            X_batch = X[:, i:i+batch_size]\n",
    "            Y_batch = Y[:, i:i+batch_size]\n",
    "\n",
    "            softmax_out, caches = forward_propagation(X_batch, parameters)\n",
    "            gradients = backward_propagation(Y_batch, softmax_out, caches)\n",
    "            update_parameters(parameters, caches, gradients, learning_rate)\n",
    "            cost = cross_entropy(softmax_out, Y_batch)\n",
    "            epoch_cost += np.squeeze(cost)\n",
    "            \n",
    "        costs.append(epoch_cost)\n",
    "        if print_cost and epoch % (epochs // 500) == 0:\n",
    "            print(\"Cost after epoch {}: {}\".format(epoch, epoch_cost))\n",
    "        if epoch % (epochs // 100) == 0:\n",
    "            learning_rate *= 0.98\n",
    "            \n",
    "    if plot_cost:\n",
    "        plt.plot(np.arange(epochs), costs)\n",
    "        plt.xlabel('# of epochs')\n",
    "        plt.ylabel('cost')\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 2.75680356264453\n",
      "Cost after epoch 10: 2.7565495917078153\n",
      "Cost after epoch 20: 2.7562847143743987\n",
      "Cost after epoch 30: 2.755992379847787\n",
      "Cost after epoch 40: 2.7556558325546927\n",
      "Cost after epoch 50: 2.75525712276606\n",
      "Cost after epoch 60: 2.7547860744644423\n",
      "Cost after epoch 70: 2.754216733264153\n",
      "Cost after epoch 80: 2.7535251162365477\n",
      "Cost after epoch 90: 2.752684993906825\n",
      "Cost after epoch 100: 2.7516664198860896\n",
      "Cost after epoch 110: 2.750459266692647\n",
      "Cost after epoch 120: 2.7490129260239016\n",
      "Cost after epoch 130: 2.747282880553974\n",
      "Cost after epoch 140: 2.745220928756987\n",
      "Cost after epoch 150: 2.742772375342675\n",
      "Cost after epoch 160: 2.7399318694608756\n",
      "Cost after epoch 170: 2.7366011967973836\n",
      "Cost after epoch 180: 2.732703597973841\n",
      "Cost after epoch 190: 2.7281614273184998\n",
      "Cost after epoch 200: 2.7228918983936126\n",
      "Cost after epoch 210: 2.7169258009122497\n",
      "Cost after epoch 220: 2.710108159180422\n",
      "Cost after epoch 230: 2.7023497577029203\n",
      "Cost after epoch 240: 2.69358123514477\n",
      "Cost after epoch 250: 2.6837482839192024\n",
      "Cost after epoch 260: 2.673025462787605\n",
      "Cost after epoch 270: 2.6612703468350354\n",
      "Cost after epoch 280: 2.6484988543875856\n",
      "Cost after epoch 290: 2.6347915046976853\n",
      "Cost after epoch 300: 2.6202772892878627\n",
      "Cost after epoch 310: 2.605411113004535\n",
      "Cost after epoch 320: 2.590180521257249\n",
      "Cost after epoch 330: 2.57479660806409\n",
      "Cost after epoch 340: 2.5595167455427563\n",
      "Cost after epoch 350: 2.5445960937771575\n",
      "Cost after epoch 360: 2.5305191146329515\n",
      "Cost after epoch 370: 2.517220732262239\n",
      "Cost after epoch 380: 2.504800175840604\n",
      "Cost after epoch 390: 2.493326768516894\n",
      "Cost after epoch 400: 2.4828093809911853\n",
      "Cost after epoch 410: 2.4733670140765427\n",
      "Cost after epoch 420: 2.464730475854249\n",
      "Cost after epoch 430: 2.4567603153688213\n",
      "Cost after epoch 440: 2.4493168350220116\n",
      "Cost after epoch 450: 2.442250723755278\n",
      "Cost after epoch 460: 2.4355344018129426\n",
      "Cost after epoch 470: 2.4289159393389097\n",
      "Cost after epoch 480: 2.422259024403729\n",
      "Cost after epoch 490: 2.415457117508685\n",
      "Cost after epoch 500: 2.4084228069515525\n",
      "Cost after epoch 510: 2.401225355734038\n",
      "Cost after epoch 520: 2.39371590714953\n",
      "Cost after epoch 530: 2.3858629251577206\n",
      "Cost after epoch 540: 2.3776717093907087\n",
      "Cost after epoch 550: 2.3691686389307347\n",
      "Cost after epoch 560: 2.3605586212363643\n",
      "Cost after epoch 570: 2.35176532272585\n",
      "Cost after epoch 580: 2.342836522174549\n",
      "Cost after epoch 590: 2.33384211302347\n",
      "Cost after epoch 600: 2.3248524492796894\n",
      "Cost after epoch 610: 2.3160945164248954\n",
      "Cost after epoch 620: 2.3074808977982197\n",
      "Cost after epoch 630: 2.2990433360574616\n",
      "Cost after epoch 640: 2.290823176376281\n",
      "Cost after epoch 650: 2.2828533189291407\n",
      "Cost after epoch 660: 2.2752951992189403\n",
      "Cost after epoch 670: 2.2680340938705674\n",
      "Cost after epoch 680: 2.261065564022577\n",
      "Cost after epoch 690: 2.2543962226938756\n",
      "Cost after epoch 700: 2.248028823593476\n",
      "Cost after epoch 710: 2.242069961957283\n",
      "Cost after epoch 720: 2.2364102408244175\n",
      "Cost after epoch 730: 2.2310322217208673\n",
      "Cost after epoch 740: 2.225929172043208\n",
      "Cost after epoch 750: 2.2210928153389067\n",
      "Cost after epoch 760: 2.2165941804331686\n",
      "Cost after epoch 770: 2.2123417662317473\n",
      "Cost after epoch 780: 2.2083151982766043\n",
      "Cost after epoch 790: 2.204502934552696\n",
      "Cost after epoch 800: 2.200893014283424\n",
      "Cost after epoch 810: 2.1975335526662607\n",
      "Cost after epoch 820: 2.194352268843334\n",
      "Cost after epoch 830: 2.1913307943464084\n",
      "Cost after epoch 840: 2.1884581349834407\n",
      "Cost after epoch 850: 2.185723733659072\n",
      "Cost after epoch 860: 2.1831636824069953\n",
      "Cost after epoch 870: 2.1807232666159098\n",
      "Cost after epoch 880: 2.1783888946233603\n",
      "Cost after epoch 890: 2.17615299725917\n",
      "Cost after epoch 900: 2.174008671357155\n",
      "Cost after epoch 910: 2.1719862448940352\n",
      "Cost after epoch 920: 2.1700446959976514\n",
      "Cost after epoch 930: 2.168175101041368\n",
      "Cost after epoch 940: 2.16637330337833\n",
      "Cost after epoch 950: 2.1646356482062226\n",
      "Cost after epoch 960: 2.1629887938056753\n",
      "Cost after epoch 970: 2.1614012721319025\n",
      "Cost after epoch 980: 2.1598673737626357\n",
      "Cost after epoch 990: 2.1583851278531614\n",
      "Cost after epoch 1000: 2.1569528275170082\n",
      "Cost after epoch 1010: 2.1555936519815586\n",
      "Cost after epoch 1020: 2.154282632036387\n",
      "Cost after epoch 1030: 2.153015865150697\n",
      "Cost after epoch 1040: 2.151792404423136\n",
      "Cost after epoch 1050: 2.1506113995228264\n",
      "Cost after epoch 1060: 2.1494923731916793\n",
      "Cost after epoch 1070: 2.1484150309266346\n",
      "Cost after epoch 1080: 2.1473763943774222\n",
      "Cost after epoch 1090: 2.146375846977879\n",
      "Cost after epoch 1100: 2.145412785399577\n",
      "Cost after epoch 1110: 2.144503099573233\n",
      "Cost after epoch 1120: 2.1436301710316665\n",
      "Cost after epoch 1130: 2.1427915133640743\n",
      "Cost after epoch 1140: 2.1419865283161172\n",
      "Cost after epoch 1150: 2.1412145956576065\n",
      "Cost after epoch 1160: 2.1404882275676487\n",
      "Cost after epoch 1170: 2.139793866153669\n",
      "Cost after epoch 1180: 2.1391293120403687\n",
      "Cost after epoch 1190: 2.1384938551739334\n",
      "Cost after epoch 1200: 2.137886745436915\n",
      "Cost after epoch 1210: 2.137317503896114\n",
      "Cost after epoch 1220: 2.1367751321371333\n",
      "Cost after epoch 1230: 2.136257591659589\n",
      "Cost after epoch 1240: 2.135763996483116\n",
      "Cost after epoch 1250: 2.135293411528514\n",
      "Cost after epoch 1260: 2.1348528504503985\n",
      "Cost after epoch 1270: 2.134433396869394\n",
      "Cost after epoch 1280: 2.1340330954172324\n",
      "Cost after epoch 1290: 2.1336508771874048\n",
      "Cost after epoch 1300: 2.1332856300890373\n",
      "Cost after epoch 1310: 2.1329424648528876\n",
      "Cost after epoch 1320: 2.1326140961597875\n",
      "Cost after epoch 1330: 2.1322986451655748\n",
      "Cost after epoch 1340: 2.131994933898485\n",
      "Cost after epoch 1350: 2.1317017642402916\n",
      "Cost after epoch 1360: 2.1314230575851894\n",
      "Cost after epoch 1370: 2.131152746954444\n",
      "Cost after epoch 1380: 2.1308890909891103\n",
      "Cost after epoch 1390: 2.1306309410544437\n",
      "Cost after epoch 1400: 2.1303771596928143\n",
      "Cost after epoch 1410: 2.1301312085892232\n",
      "Cost after epoch 1420: 2.1298878576797886\n",
      "Cost after epoch 1430: 2.1296455851399596\n",
      "Cost after epoch 1440: 2.129403416952834\n",
      "Cost after epoch 1450: 2.129160417489935\n",
      "Cost after epoch 1460: 2.128920208805629\n",
      "Cost after epoch 1470: 2.1286780448168514\n",
      "Cost after epoch 1480: 2.1284326776725173\n",
      "Cost after epoch 1490: 2.128183408522482\n",
      "Cost after epoch 1500: 2.1279295930609274\n",
      "Cost after epoch 1510: 2.127675438611036\n",
      "Cost after epoch 1520: 2.1274163777694843\n",
      "Cost after epoch 1530: 2.127151445250459\n",
      "Cost after epoch 1540: 2.126880256191452\n",
      "Cost after epoch 1550: 2.1266024850523544\n",
      "Cost after epoch 1560: 2.12632313240272\n",
      "Cost after epoch 1570: 2.126037580762032\n",
      "Cost after epoch 1580: 2.1257451185304688\n",
      "Cost after epoch 1590: 2.125445661545335\n",
      "Cost after epoch 1600: 2.1251391811763103\n",
      "Cost after epoch 1610: 2.1248314857287967\n",
      "Cost after epoch 1620: 2.1245177741083756\n",
      "Cost after epoch 1630: 2.124197538831613\n",
      "Cost after epoch 1640: 2.123870948936486\n",
      "Cost after epoch 1650: 2.1235382197733066\n",
      "Cost after epoch 1660: 2.123205832045616\n",
      "Cost after epoch 1670: 2.122868763400559\n",
      "Cost after epoch 1680: 2.122526654298049\n",
      "Cost after epoch 1690: 2.1221798613815457\n",
      "Cost after epoch 1700: 2.1218287755375775\n",
      "Cost after epoch 1710: 2.1214803137776412\n",
      "Cost after epoch 1720: 2.12112927695306\n",
      "Cost after epoch 1730: 2.12077539999583\n",
      "Cost after epoch 1740: 2.120419155868857\n",
      "Cost after epoch 1750: 2.120061038911356\n",
      "Cost after epoch 1760: 2.1197081132500477\n",
      "Cost after epoch 1770: 2.119355108058382\n",
      "Cost after epoch 1780: 2.119001807750754\n",
      "Cost after epoch 1790: 2.118648733987468\n",
      "Cost after epoch 1800: 2.1182964177514196\n",
      "Cost after epoch 1810: 2.117951770214609\n",
      "Cost after epoch 1820: 2.1176095909423256\n",
      "Cost after epoch 1830: 2.1172696805825577\n",
      "Cost after epoch 1840: 2.1169325521494637\n",
      "Cost after epoch 1850: 2.116598717854081\n",
      "Cost after epoch 1860: 2.116274658633612\n",
      "Cost after epoch 1870: 2.115955400843856\n",
      "Cost after epoch 1880: 2.115640740827139\n",
      "Cost after epoch 1890: 2.115331139555853\n",
      "Cost after epoch 1900: 2.1150270495467525\n",
      "Cost after epoch 1910: 2.114734290203824\n",
      "Cost after epoch 1920: 2.114448265813382\n",
      "Cost after epoch 1930: 2.1141687593906653\n",
      "Cost after epoch 1940: 2.113896151143149\n",
      "Cost after epoch 1950: 2.1136308076556896\n",
      "Cost after epoch 1960: 2.113377714148084\n",
      "Cost after epoch 1970: 2.113132783349016\n",
      "Cost after epoch 1980: 2.11289578464058\n",
      "Cost after epoch 1990: 2.1126670018168405\n",
      "Cost after epoch 2000: 2.1124467020164284\n",
      "Cost after epoch 2010: 2.112238926144421\n",
      "Cost after epoch 2020: 2.1120401947171805\n",
      "Cost after epoch 2030: 2.1118502684359832\n",
      "Cost after epoch 2040: 2.1116693292948767\n",
      "Cost after epoch 2050: 2.111497541248773\n",
      "Cost after epoch 2060: 2.111337951066525\n",
      "Cost after epoch 2070: 2.1111877495674714\n",
      "Cost after epoch 2080: 2.1110466958772958\n",
      "Cost after epoch 2090: 2.11091487218343\n",
      "Cost after epoch 2100: 2.1107923424761905\n",
      "Cost after epoch 2110: 2.1106811642339323\n",
      "Cost after epoch 2120: 2.1105792229851446\n",
      "Cost after epoch 2130: 2.1104862834616447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 2140: 2.110402335234635\n",
      "Cost after epoch 2150: 2.1103273504810076\n",
      "Cost after epoch 2160: 2.110262449682369\n",
      "Cost after epoch 2170: 2.1102062016065912\n",
      "Cost after epoch 2180: 2.110158382967751\n",
      "Cost after epoch 2190: 2.1101189018320277\n",
      "Cost after epoch 2200: 2.1100876505168356\n",
      "Cost after epoch 2210: 2.1100649042805126\n",
      "Cost after epoch 2220: 2.1100498725021457\n",
      "Cost after epoch 2230: 2.1100423494689524\n",
      "Cost after epoch 2240: 2.1100421761344124\n",
      "Cost after epoch 2250: 2.1100491801360697\n",
      "Cost after epoch 2260: 2.110062914102629\n",
      "Cost after epoch 2270: 2.1100831604663557\n",
      "Cost after epoch 2280: 2.110109736289048\n",
      "Cost after epoch 2290: 2.110142432585697\n",
      "Cost after epoch 2300: 2.11018103018284\n",
      "Cost after epoch 2310: 2.110224502934751\n",
      "Cost after epoch 2320: 2.110273120792433\n",
      "Cost after epoch 2330: 2.1103267286783387\n",
      "Cost after epoch 2340: 2.110385086831005\n",
      "Cost after epoch 2350: 2.110447948928492\n",
      "Cost after epoch 2360: 2.110513864718593\n",
      "Cost after epoch 2370: 2.1105834952740516\n",
      "Cost after epoch 2380: 2.110656718227572\n",
      "Cost after epoch 2390: 2.110733282923257\n",
      "Cost after epoch 2400: 2.1108129359731693\n",
      "Cost after epoch 2410: 2.1108939576374994\n",
      "Cost after epoch 2420: 2.1109772997362817\n",
      "Cost after epoch 2430: 2.1110628767170825\n",
      "Cost after epoch 2440: 2.1111504460088373\n",
      "Cost after epoch 2450: 2.111239765980622\n",
      "Cost after epoch 2460: 2.1113289916010127\n",
      "Cost after epoch 2470: 2.1114192674456955\n",
      "Cost after epoch 2480: 2.1115105474561857\n",
      "Cost after epoch 2490: 2.111602613564645\n",
      "Cost after epoch 2500: 2.1116952518203194\n",
      "Cost after epoch 2510: 2.1117866168493777\n",
      "Cost after epoch 2520: 2.1118779559382532\n",
      "Cost after epoch 2530: 2.1119692632041596\n",
      "Cost after epoch 2540: 2.1120603576044923\n",
      "Cost after epoch 2550: 2.1121510646416652\n",
      "Cost after epoch 2560: 2.1122396379261654\n",
      "Cost after epoch 2570: 2.1123273498574977\n",
      "Cost after epoch 2580: 2.1124142331638094\n",
      "Cost after epoch 2590: 2.1125001516953645\n",
      "Cost after epoch 2600: 2.112584977396435\n",
      "Cost after epoch 2610: 2.1126671331340963\n",
      "Cost after epoch 2620: 2.1127478544746308\n",
      "Cost after epoch 2630: 2.112827209097224\n",
      "Cost after epoch 2640: 2.11290510886125\n",
      "Cost after epoch 2650: 2.1129814743879103\n",
      "Cost after epoch 2660: 2.113054938542048\n",
      "Cost after epoch 2670: 2.113126655525646\n",
      "Cost after epoch 2680: 2.1131967227103177\n",
      "Cost after epoch 2690: 2.113265098815049\n",
      "Cost after epoch 2700: 2.1133317512092136\n",
      "Cost after epoch 2710: 2.1133955360383903\n",
      "Cost after epoch 2720: 2.113457499338412\n",
      "Cost after epoch 2730: 2.1135177620181587\n",
      "Cost after epoch 2740: 2.1135763251284554\n",
      "Cost after epoch 2750: 2.113633197648058\n",
      "Cost after epoch 2760: 2.1136874487808583\n",
      "Cost after epoch 2770: 2.1137400063256577\n",
      "Cost after epoch 2780: 2.1137910084239264\n",
      "Cost after epoch 2790: 2.113840491628916\n",
      "Cost after epoch 2800: 2.113888499283703\n",
      "Cost after epoch 2810: 2.1139342849930376\n",
      "Cost after epoch 2820: 2.1139786629292807\n",
      "Cost after epoch 2830: 2.1140217826695236\n",
      "Cost after epoch 2840: 2.114063708215516\n",
      "Cost after epoch 2850: 2.1141045089953123\n",
      "Cost after epoch 2860: 2.114143581394566\n",
      "Cost after epoch 2870: 2.114181642485583\n",
      "Cost after epoch 2880: 2.114218848474953\n",
      "Cost after epoch 2890: 2.1142552824770164\n",
      "Cost after epoch 2900: 2.114291031606986\n",
      "Cost after epoch 2910: 2.1143255857424608\n",
      "Cost after epoch 2920: 2.1143595901689602\n",
      "Cost after epoch 2930: 2.114393204184615\n",
      "Cost after epoch 2940: 2.1144265221315144\n",
      "Cost after epoch 2950: 2.11445964099102\n",
      "Cost after epoch 2960: 2.114492091878842\n",
      "Cost after epoch 2970: 2.1145244792235522\n",
      "Cost after epoch 2980: 2.114556963226621\n",
      "Cost after epoch 2990: 2.1145896425245247\n",
      "Cost after epoch 3000: 2.114622617180699\n",
      "Cost after epoch 3010: 2.1146554082038618\n",
      "Cost after epoch 3020: 2.114688612758739\n",
      "Cost after epoch 3030: 2.1147223910338657\n",
      "Cost after epoch 3040: 2.114756840259803\n",
      "Cost after epoch 3050: 2.1147920580794892\n",
      "Cost after epoch 3060: 2.1148275079052286\n",
      "Cost after epoch 3070: 2.1148638133381654\n",
      "Cost after epoch 3080: 2.1149011347200215\n",
      "Cost after epoch 3090: 2.1149395635066752\n",
      "Cost after epoch 3100: 2.1149791907673983\n",
      "Cost after epoch 3110: 2.115019380719089\n",
      "Cost after epoch 3120: 2.1150608151498402\n",
      "Cost after epoch 3130: 2.1151036555502447\n",
      "Cost after epoch 3140: 2.1151479845246124\n",
      "Cost after epoch 3150: 2.115193883702275\n",
      "Cost after epoch 3160: 2.1152405836079713\n",
      "Cost after epoch 3170: 2.1152888524938787\n",
      "Cost after epoch 3180: 2.1153388545505485\n",
      "Cost after epoch 3190: 2.1153906616134015\n",
      "Cost after epoch 3200: 2.1154443441448723\n",
      "Cost after epoch 3210: 2.1154989721041213\n",
      "Cost after epoch 3220: 2.1155554240018892\n",
      "Cost after epoch 3230: 2.11561386857207\n",
      "Cost after epoch 3240: 2.1156743659388275\n",
      "Cost after epoch 3250: 2.1157369746168735\n",
      "Cost after epoch 3260: 2.115800584729421\n",
      "Cost after epoch 3270: 2.1158662038077094\n",
      "Cost after epoch 3280: 2.115934007034457\n",
      "Cost after epoch 3290: 2.116004042650859\n",
      "Cost after epoch 3300: 2.116076357181574\n",
      "Cost after epoch 3310: 2.116149648832459\n",
      "Cost after epoch 3320: 2.1162250677182284\n",
      "Cost after epoch 3330: 2.11630279725304\n",
      "Cost after epoch 3340: 2.1163828741938837\n",
      "Cost after epoch 3350: 2.1164653335719246\n",
      "Cost after epoch 3360: 2.116548676108548\n",
      "Cost after epoch 3370: 2.116634202999071\n",
      "Cost after epoch 3380: 2.1167221074134335\n",
      "Cost after epoch 3390: 2.1168124154132753\n",
      "Cost after epoch 3400: 2.1169051513943735\n",
      "Cost after epoch 3410: 2.1169986187177163\n",
      "Cost after epoch 3420: 2.1170942734405807\n",
      "Cost after epoch 3430: 2.117192319667405\n",
      "Cost after epoch 3440: 2.1172927737914096\n",
      "Cost after epoch 3450: 2.1173956506450615\n",
      "Cost after epoch 3460: 2.117499061160859\n",
      "Cost after epoch 3470: 2.1176046158161315\n",
      "Cost after epoch 3480: 2.117712530438116\n",
      "Cost after epoch 3490: 2.1178228128961023\n",
      "Cost after epoch 3500: 2.117935469629693\n",
      "Cost after epoch 3510: 2.1180484280039797\n",
      "Cost after epoch 3520: 2.1181634488229846\n",
      "Cost after epoch 3530: 2.118280760024879\n",
      "Cost after epoch 3540: 2.1184003621272094\n",
      "Cost after epoch 3550: 2.118522254360447\n",
      "Cost after epoch 3560: 2.11864419248266\n",
      "Cost after epoch 3570: 2.1187680804808227\n",
      "Cost after epoch 3580: 2.1188941584089105\n",
      "Cost after epoch 3590: 2.119022420570148\n",
      "Cost after epoch 3600: 2.1191528601246854\n",
      "Cost after epoch 3610: 2.1192830755288266\n",
      "Cost after epoch 3620: 2.1194151043514515\n",
      "Cost after epoch 3630: 2.119549198419444\n",
      "Cost after epoch 3640: 2.119685346884695\n",
      "Cost after epoch 3650: 2.1198235378947965\n",
      "Cost after epoch 3660: 2.119961228588723\n",
      "Cost after epoch 3670: 2.1201005786182474\n",
      "Cost after epoch 3680: 2.1202418509394585\n",
      "Cost after epoch 3690: 2.120385030519629\n",
      "Cost after epoch 3700: 2.12053010145143\n",
      "Cost after epoch 3710: 2.1206743966465744\n",
      "Cost after epoch 3720: 2.120820184952407\n",
      "Cost after epoch 3730: 2.1209677395622015\n",
      "Cost after epoch 3740: 2.121117042120316\n",
      "Cost after epoch 3750: 2.121268073515275\n",
      "Cost after epoch 3760: 2.121418060142701\n",
      "Cost after epoch 3770: 2.1215693662215855\n",
      "Cost after epoch 3780: 2.12172227409556\n",
      "Cost after epoch 3790: 2.121876762842936\n",
      "Cost after epoch 3800: 2.1220328108935704\n",
      "Cost after epoch 3810: 2.1221875560566534\n",
      "Cost after epoch 3820: 2.1223434435535196\n",
      "Cost after epoch 3830: 2.122500763647779\n",
      "Cost after epoch 3840: 2.122659493509465\n",
      "Cost after epoch 3850: 2.122819609756539\n",
      "Cost after epoch 3860: 2.122978179430191\n",
      "Cost after epoch 3870: 2.1231377141715173\n",
      "Cost after epoch 3880: 2.123298510837114\n",
      "Cost after epoch 3890: 2.1234605452561057\n",
      "Cost after epoch 3900: 2.1236237927917667\n",
      "Cost after epoch 3910: 2.123785267158066\n",
      "Cost after epoch 3920: 2.123947531896931\n",
      "Cost after epoch 3930: 2.1241108890757534\n",
      "Cost after epoch 3940: 2.1242753136695205\n",
      "Cost after epoch 3950: 2.1244407802644667\n",
      "Cost after epoch 3960: 2.1246042660981184\n",
      "Cost after epoch 3970: 2.1247683723849073\n",
      "Cost after epoch 3980: 2.1249334050010527\n",
      "Cost after epoch 3990: 2.1250993384833126\n",
      "Cost after epoch 4000: 2.1252661470486918\n",
      "Cost after epoch 4010: 2.1254307875524567\n",
      "Cost after epoch 4020: 2.125595885126544\n",
      "Cost after epoch 4030: 2.1257617480637645\n",
      "Cost after epoch 4040: 2.125928350816604\n",
      "Cost after epoch 4050: 2.1260956675796363\n",
      "Cost after epoch 4060: 2.1262606500354146\n",
      "Cost after epoch 4070: 2.126425934095959\n",
      "Cost after epoch 4080: 2.1265918291120363\n",
      "Cost after epoch 4090: 2.1267583097516995\n",
      "Cost after epoch 4100: 2.126925350480537\n",
      "Cost after epoch 4110: 2.127089912037749\n",
      "Cost after epoch 4120: 2.127254628706648\n",
      "Cost after epoch 4130: 2.127419809589821\n",
      "Cost after epoch 4140: 2.1275854298232084\n",
      "Cost after epoch 4150: 2.127751464389971\n",
      "Cost after epoch 4160: 2.127914896267094\n",
      "Cost after epoch 4170: 2.128078346511703\n",
      "Cost after epoch 4180: 2.128242122739042\n",
      "Cost after epoch 4190: 2.1284062007632607\n",
      "Cost after epoch 4200: 2.1285705562901436\n",
      "Cost after epoch 4210: 2.128732206622764\n",
      "Cost after epoch 4220: 2.128893748864365\n",
      "Cost after epoch 4230: 2.129055487981379\n",
      "Cost after epoch 4240: 2.1292174006390865\n",
      "Cost after epoch 4250: 2.129379463433927\n",
      "Cost after epoch 4260: 2.129538738965978\n",
      "Cost after epoch 4270: 2.129697790564525\n",
      "Cost after epoch 4280: 2.1298569194710146\n",
      "Cost after epoch 4290: 2.130016103341252\n",
      "Cost after epoch 4300: 2.1301753197971385\n",
      "Cost after epoch 4310: 2.1303316865819903\n",
      "Cost after epoch 4320: 2.130487724358977\n",
      "Cost after epoch 4330: 2.130643729658661\n",
      "Cost after epoch 4340: 2.1307996812365886\n",
      "Cost after epoch 4350: 2.1309555578450112\n",
      "Cost after epoch 4360: 2.13110854109958\n",
      "Cost after epoch 4370: 2.131261101038468\n",
      "Cost after epoch 4380: 2.1314135285885936\n",
      "Cost after epoch 4390: 2.131565803687266\n",
      "Cost after epoch 4400: 2.1317179062950258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 4410: 2.131867089532102\n",
      "Cost after epoch 4420: 2.1320157657787515\n",
      "Cost after epoch 4430: 2.1321642195597392\n",
      "Cost after epoch 4440: 2.1323124320517257\n",
      "Cost after epoch 4450: 2.1324603844772834\n",
      "Cost after epoch 4460: 2.1326054080276715\n",
      "Cost after epoch 4470: 2.1327498513002965\n",
      "Cost after epoch 4480: 2.132893991713198\n",
      "Cost after epoch 4490: 2.1330378117181397\n",
      "Cost after epoch 4500: 2.1331812938318806\n",
      "Cost after epoch 4510: 2.1333218528574998\n",
      "Cost after epoch 4520: 2.133461768365667\n",
      "Cost after epoch 4530: 2.1336013100555866\n",
      "Cost after epoch 4540: 2.1337404616705493\n",
      "Cost after epoch 4550: 2.133879207034576\n",
      "Cost after epoch 4560: 2.1340150491251784\n",
      "Cost after epoch 4570: 2.134150194089178\n",
      "Cost after epoch 4580: 2.1342849033848705\n",
      "Cost after epoch 4590: 2.1344191620465756\n",
      "Cost after epoch 4600: 2.134552955202004\n",
      "Cost after epoch 4610: 2.134683877640858\n",
      "Cost after epoch 4620: 2.1348140584955586\n",
      "Cost after epoch 4630: 2.134943750548264\n",
      "Cost after epoch 4640: 2.1350729401092257\n",
      "Cost after epoch 4650: 2.1352016135919536\n",
      "Cost after epoch 4660: 2.1353274603689654\n",
      "Cost after epoch 4670: 2.135452529729449\n",
      "Cost after epoch 4680: 2.1355770654272135\n",
      "Cost after epoch 4690: 2.135701055020968\n",
      "Cost after epoch 4700: 2.135824486180034\n",
      "Cost after epoch 4710: 2.135945144824015\n",
      "Cost after epoch 4720: 2.1360649982834854\n",
      "Cost after epoch 4730: 2.136184281010368\n",
      "Cost after epoch 4740: 2.1363029817740804\n",
      "Cost after epoch 4750: 2.1364210894597573\n",
      "Cost after epoch 4760: 2.136536487754683\n",
      "Cost after epoch 4770: 2.1366510605781985\n",
      "Cost after epoch 4780: 2.1367650328807906\n",
      "Cost after epoch 4790: 2.1368783945964926\n",
      "Cost after epoch 4800: 2.1369911357782017\n",
      "Cost after epoch 4810: 2.1371012384212427\n",
      "Cost after epoch 4820: 2.1372105021918064\n",
      "Cost after epoch 4830: 2.137319142408364\n",
      "Cost after epoch 4840: 2.137427150116933\n",
      "Cost after epoch 4850: 2.1375345164838286\n",
      "Cost after epoch 4860: 2.1376393217358887\n",
      "Cost after epoch 4870: 2.137743281002279\n",
      "Cost after epoch 4880: 2.137846599902667\n",
      "Cost after epoch 4890: 2.137949270537487\n",
      "Cost after epoch 4900: 2.1380512851274416\n",
      "Cost after epoch 4910: 2.1381508214998712\n",
      "Cost after epoch 4920: 2.138249510468607\n",
      "Cost after epoch 4930: 2.138347547946213\n",
      "Cost after epoch 4940: 2.1384449270264834\n",
      "Cost after epoch 4950: 2.138541640922206\n",
      "Cost after epoch 4960: 2.138635963936542\n",
      "Cost after epoch 4970: 2.1387294432435815\n",
      "Cost after epoch 4980: 2.1388222650936397\n",
      "Cost after epoch 4990: 2.1389144235105824\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x266b3f772b0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def word2vec(vocab, )\n",
    "doc = \"After the deduction of the costs of investing, \" \\\n",
    "      \"beating the stock market is a loser's game.\"\n",
    "tokens = nltk.word_tokenize(doc)\n",
    "word_to_id, id_to_word = mapping(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_id, 3)\n",
    "vocab_size = len(id_to_word)\n",
    "m = Y.shape[1]\n",
    "# turn Y into one hot encoding\n",
    "Y_one_hot = np.zeros((vocab_size, m))\n",
    "Y_one_hot[Y.flatten(), np.arange(m)] = 1\n",
    "\n",
    "paras = skipgram_model_training(X, Y_one_hot, vocab_size, 50, 0.05, 5000, batch_size=128, parameters=None, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.arange(vocab_size)\n",
    "X_test = np.expand_dims(X_test, axis=0)\n",
    "softmax_test, _ = forward_propagation(X_test, paras)\n",
    "top_sorted_inds = np.argsort(softmax_test, axis=0)[-4:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beating's neighbor words: ['market', 'stock', 'the', 'investing']\n",
      "of's neighbor words: ['the', 'costs', 'beating', 'of']\n",
      "investing's neighbor words: ['the', 'of', ',', 'beating']\n",
      "stock's neighbor words: ['a', 'is', ',', 'market']\n",
      "market's neighbor words: ['loser', 'stock', 'is', 'beating']\n",
      "the's neighbor words: ['is', 'market', 'stock', 'the']\n",
      "'s's neighbor words: ['.', 'a', 'game', \"'s\"]\n",
      "deduction's neighbor words: ['costs', 'After', 'the', 'of']\n",
      ",'s neighbor words: ['stock', 'the', 'beating', 'costs']\n",
      ".'s neighbor words: ['loser', \"'s\", 'game', 'stock']\n",
      "a's neighbor words: ['loser', \"'s\", 'game', 'stock']\n",
      "is's neighbor words: ['loser', \"'s\", 'market', 'a']\n",
      "game's neighbor words: ['.', 'a', 'game', \"'s\"]\n",
      "After's neighbor words: ['of', 'the', 'deduction', 'costs']\n",
      "loser's neighbor words: ['.', 'a', 'game', \"'s\"]\n",
      "costs's neighbor words: [',', 'of', 'deduction', 'the']\n"
     ]
    }
   ],
   "source": [
    "for input_ind in range(vocab_size):\n",
    "    input_word = id_to_word[input_ind]\n",
    "    output_words = [id_to_word[output_ind] for output_ind in top_sorted_inds[::-1, input_ind]]\n",
    "    print(\"{}'s neighbor words: {}\".format(input_word, output_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_to_ind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2fdbdaa694e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoftmax_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_ind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'market stock'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-2fdbdaa694e7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msoftmax_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_to_ind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;34m'market stock'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparas\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'word_to_ind' is not defined"
     ]
    }
   ],
   "source": [
    "softmax_test, _ = forward_propagation(np.array([[word_to_ind(i) for i in 'market stock']]), paras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'investing': 0, 'stock': 1, 'market': 2, 'costs': 3, 'game': 4, 'is': 5, 'beating': 6, 'loser': 7, '.': 8, 'the': 9, 'a': 10, \"'s\": 11, ',': 12, 'After': 13, 'deduction': 14, 'of': 15}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.36258346e+00, -1.51939799e-01,  1.79864933e+00,  7.46485077e-02,\n",
       "        1.71707055e+00,  1.39494030e+00, -2.43966187e-01,  1.10499878e+00,\n",
       "        1.93225486e+00,  1.10024962e+00,  1.42644762e+00, -3.44037421e-02,\n",
       "        2.23996543e+00,  6.47172906e-01,  1.16378760e+00, -4.82243024e-02,\n",
       "        1.24532815e+00,  1.35382484e+00,  4.23859775e-01,  9.64613092e-01,\n",
       "        8.27896487e-01,  2.85272980e-01, -1.03334034e+00,  9.09670740e-01,\n",
       "        4.01100181e-01, -2.30575056e-01, -1.51392288e-01,  3.16129068e+00,\n",
       "       -1.14181876e+00,  1.60410782e-01,  1.52513278e+00,  1.07826358e+00,\n",
       "       -1.26684035e-01, -1.17992474e+00, -8.34411849e-02,  8.44996298e-01,\n",
       "       -1.02043837e-01, -1.27448646e+00, -1.53905513e-01, -4.28818251e-01,\n",
       "        3.45002407e-01,  5.61200505e-01, -1.39268048e+00,  8.08054556e-01,\n",
       "        6.63052658e-01,  1.57858403e-03,  5.96344511e-01,  8.39148903e-02,\n",
       "       -2.27390801e-01, -7.28294404e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(forward_propagation(np.array([[word_to_id[i] for i in ['market', 'loser']]]), paras)[1]['word_vec'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
